<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[@vishnuatrai]]></title>
  <link href="http://vishnuatrai.in/atom.xml" rel="self"/>
  <link href="http://vishnuatrai.in/"/>
  <updated>2020-06-28T15:48:36+05:30</updated>
  <id>http://vishnuatrai.in/</id>
  <author>
    <name><![CDATA[vishnuatrai.in]]></name>
    <email><![CDATA[vishnu.atrai@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[MapR-DB - Querying Pro-tips]]></title>
    <link href="http://vishnuatrai.in/blog/2020/05/03/mapr-db-querying-pro-tips/"/>
    <updated>2020-05-03T13:09:15+05:30</updated>
    <id>http://vishnuatrai.in/blog/2020/05/03/mapr-db-querying-pro-tips</id>
    <content type="html"><![CDATA[<h3>What is MapR?</h3>

<p>MapR is a complete enterprise-grade distribution for Apache Hadoop. The MapR Data Platform has been developed to improve Hadoop’s reliability, performance, and ease of use. The MapR provides a full Hadoop stack that includes the MapR File System (MapR-FS), the MapR-DB NoSQL database management system, MapR Streams, the MapR Control System (MCS) user interface, and a full family of Hadoop ecosystem projects.<!--more--></p>

<h3>Querying</h3>

<p>MapR provides native JSON (JavaScript Object Notation) support to MapR Database. The JSON access layer is known as OJAI(Open JSON Application Interface).</p>

<h4>Querying dependency</h4>

<p>The workstation must setup MapR client and its certificates and should be able to login to <code>mapr dbshell</code></p>

<h4>Java program setup</h4>

<p>The workstation must setup MapR client and its certificates</p>

<pre><code>pom.xml
&lt;dependency&gt;
    &lt;groupId&gt;org.ojai&lt;/groupId&gt;
    &lt;artifactId&gt;ojai&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;com.mapr.db&lt;/groupId&gt;
    &lt;artifactId&gt;maprdb&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>

<h4>1. Find by id</h4>

<pre><code>mapr dbshell&gt; find /datalake/path/to/table/users --c {"$eq":{"_id":"5e1dcaaea97bd2004c3caccf"}}
or 
mapr dbshell&gt; find /datalake/path/to/table/users --q {"$where":{"$and":[{"$eq":{"_id":"5e1dcaaea97bd2004c3caccf"}}]}}
#Sample Java snippets
....
com.mapr.db.Admin admin = MapRDB.newAdmin();
com.mapr.db.Table maprTable = MapRDB.getTable("/datalake/path/to/table/users");
org.ojai.Document doc = maprTable.findById("5e1dcaaea97bd2004c3caccf", fieldPaths={"_id","username","fullname","age","dob"});
....
</code></pre>

<h4>2. Query with <code>$and</code> and <code>select</code> fields</h4>

<pre><code>mapr dbshell&gt; find /datalake/path/to/table/users --q {"$select":["_id","username","fullname","age","dob"],"$where":{"$and":[{"$eq":{"_id":"5e1dcaaea97bd2004c3caccf"}}]}}
#Sample Java snippets
....
import org.ojai.store.QueryCondition.Op;
org.ojai.store.QueryCondition query = MapRDB.newCondition().and().is("_id", Op.EQUAL, "5e1dcaaea97bd2004c3caccf").close().build();
org.ojai.DocumentStream doc = maprTable.find(query, fieldPaths={"_id","username","fullname","age","dob"});
....
</code></pre>

<h4>3. Query with conditions and <code>orderBy</code></h4>

<pre><code>mapr dbshell&gt; find /datalake/path/to/table/users --q {"$where":{"$and":[{"$ge":{"dob":{"$date":"2000-01-15"}}}]},"$orderby":{"username":"desc"}}
#Sample Java snippets
org.ojai.types.OTimestamp ojaiDOB = new org.ojai.types.OTimestamp("2000-01-15");
org.ojai.store.QueryCondition condition = MapRDB.newCondition().and().is("dob", Op.GREATER_OR_EQUAL, ojaiDOB).close().build();
org.ojai.store.Query query = MapRDB.newQuery().where(condition).orderBy("username","DESC")
org.ojai.DocumentStream docs = maprTable.findQuery(query, fieldPaths={"_id","username","fullname","age","dob"});
</code></pre>

<h4>4. Query with conditions and <code>limit</code> results</h4>

<pre><code>mapr dbshell&gt; find /datalake/path/to/table/users --q {"$where":{"$and":[{"$ge":{"dob":{"$date":"2000-01-15"}}}]},"$limit":10}
#Sample Java snippets
org.ojai.types.OTimestamp ojaiDOB = new org.ojai.types.OTimestamp("2000-01-15");
org.ojai.store.QueryCondition condition = MapRDB.newCondition().and().is("dob", Op.GREATER_OR_EQUAL, ojaiDOB).close().build();
org.ojai.store.Query query = MapRDB.newQuery().where(condition).limit(10)
org.ojai.DocumentStream docs = maprTable.findQuery(query, fieldPaths={"_id","username","fullname","age","dob"});
</code></pre>

<h4>5. Query with range conditions with <code>$ge</code>,<code>$gt</code>,<code>$le</code>,<code>$lt</code></h4>

<pre><code>mapr dbshell&gt; find /datalake/path/to/table/users --q {"$where":{"$and":[{"$ge":{"dob":{"$date":"2000-01-15"}}},{"$le":{"dob":{"$date":"2005-01-15"}}}]},"$orderby":{"username":"desc"},"$limit":10}
#Sample Java snippets
org.ojai.types.OTimestamp ojaiFromDOB = new org.ojai.types.OTimestamp("2000-01-15");
org.ojai.types.OTimestamp ojaiToDOB = new org.ojai.types.OTimestamp("2005-01-15");
org.ojai.store.QueryCondition condition = MapRDB.newCondition().and().is("dob", Op.GREATER_OR_EQUAL, ojaiFromDOB).is("dob", Op.LESS_OR_EQUAL, ojaiToDOB).close().build();
org.ojai.store.Query query = MapRDB.newQuery().where(condition).orderBy("username","DESC").limit(10)
org.ojai.DocumentStream docs = maprTable.findQuery(query, fieldPaths={"_id","username","fullname","age","dob"});    
</code></pre>

<h4>6. Query with <code>$or</code></h4>

<pre><code>mapr dbshell&gt; find /datalake/path/to/table/users --q {"$where":{"$or":[{"$le":{"dob":{"$date":"2000-01-15"}}},{"$ge":{"age":{"$number":"20"}}}]},"$orderby":{"username":"desc"},"$limit":10}
#Sample Java snippets
org.ojai.types.OTimestamp ojaiFromDOB = new org.ojai.types.OTimestamp("2000-01-15");
org.ojai.store.QueryCondition condition = MapRDB.newCondition().or().is("dob", Op.GREATER_OR_EQUAL, ojaiFromDOB).is("age", Op.LESS_OR_EQUAL, 20).close().build();
org.ojai.store.Query query = MapRDB.newQuery().where(condition).orderBy("username","DESC").limit(10)
org.ojai.DocumentStream docs = maprTable.findQuery(query, fieldPaths={"_id","username","fullname","age","dob"});    
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[React.js - HOCs vs Render Props vs Custom Hooks Patterns]]></title>
    <link href="http://vishnuatrai.in/blog/2020/02/02/react-dot-js-hocs-vs-render-props-vs-custom-hooks-patterns/"/>
    <updated>2020-02-02T11:57:42+05:30</updated>
    <id>http://vishnuatrai.in/blog/2020/02/02/react-dot-js-hocs-vs-render-props-vs-custom-hooks-patterns</id>
    <content type="html"><![CDATA[<p>Higher-Order Components(HOCs), Render Props and Custom Hooks are techniques to share common functionality between components. It&rsquo;s recommended to always go with hooks wherever possible. HOCs and render props patterns require you to restructure your components when you use them, which can be cumbersome and make code harder to follow.</p>

<h3>Higher-Order Components</h3>

<p>Higher-order component is a function that takes a component as an argument and returns a new component. HOC adds additional data and functionality to original component so new component also referred to as Enhanced component.<!--more--></p>

<pre><code>const NewComponent = higherOrderComponent(OriginalComponent)
const EnhancedComponent = higherOrderComponent(WrappedComponent)
</code></pre>

<p>Other possibilities with HOCs,<br />
1. Passing down the props using spread operator <br />
2. Passing parameters to higher order functions</p>

<pre><code>#example 
const withAdditionalFunctionality = (WrappedComponent, additionalArgs) =&gt; {
    class WithAdditionalFunctionality extends React.Component {
        constructor(props){
            super(props)
            this.state = { 
                state1: 'value1'
                ... 
            }
        }
        additionalFunctionality = () =&gt; { 
            //additionalArgs can be use here 
            ... 
        }
        render(){
            return( &lt;WrappedComponent
                        state1={this.state.state1} 
                        additionalFunctionality={this.additionalFunctionality} 
                        {...this.props} 
                    /&gt; )
        }
    }
    return WithAdditionalFunctionality;
}
export default withAdditionalFunctionality;
</code></pre>

<h3>Render Props Pattern</h3>

<p>Render Props is another pattern for sharing code between React components using a prop whose value is a function.</p>

<pre><code>class RenderProps extends React.Component {
        constructor(props){
            super(props)
            this.state = { 
                state1: 'value1'
                ... 
            }
        }
        additionalFunctionality = () =&gt; { 
            //props.additionalArgs
            ... 
        }
        render(){
            return(&lt;div&gt;
                    { this.props.render( state1={this.state.state1}
                        additionalFunctionality={this.additionalFunctionality} ) }
                &lt;/div&gt;)
        }
    }
    export default RenderProps;
    #Usage
    &lt;RenderProps 
        render={ (state1, additionalFunctionality) =&gt; { return &lt;div&gt; .... &lt;/div&gt; } } 
    /&gt;
    &lt;RenderProps 
        render={ (state1, additionalFunctionality) =&gt; { return &lt;p&gt; .... &lt;/p&gt; } } 
    /&gt;
</code></pre>

<h3>Custom Hooks</h3>

<p>Simpler alternative to HOCs and Render Props to share the common logic between components. A custom hook can also call other hooks if required. With Hooks, you can extract stateful logic from a component so it can be tested independently and reused. Hooks allow you to reuse stateful logic without changing your component hierarchy.</p>

<pre><code>import {useState} from 'react'
function useCustomHook(arguments){
    const [state1, setState1] = useState(arguments.initialState)
    const additionalFunctionality = () =&gt; {
       //arguments can be used here
       .....
    }
    ....
    return [state1, additionalFunctionality];
}
export default useCustomHook;
#Usage
function ConsumerComponent {
    const [state1, additionalFunctionality] = useCustomHook(initialValues and args...)
    render(
        &lt;div&gt;
            ....
        &lt;/div&gt;
    )
}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySQL - Encryption at rest]]></title>
    <link href="http://vishnuatrai.in/blog/2020/01/11/mysql-encryption-at-rest/"/>
    <updated>2020-01-11T18:20:46+05:30</updated>
    <id>http://vishnuatrai.in/blog/2020/01/11/mysql-encryption-at-rest</id>
    <content type="html"><![CDATA[<p>Without encryption of data at rest, system role with access to file system can view data even without proper database permissions. A proper authentication can be used to protect data used by an application but data sitting in file system has been area of risk.</p>

<p>With encryption at rest, it has been possible to protect data in transit when moving over the network. Data at Rest Encryption is a requirement for HIPAA, PCI regulations. <!--more--></p>

<h3>Data Files at Risk</h3>

<p>When insert queries run to store data in tables, the data will be stored in respective <code>dataDir</code> ie <code>/usr/local/var/mysql</code>. A system user that has filesystem access can use <code>strings</code> or <code>xxd</code> commands to view content in data files, bin logs or redo logs.</p>

<p>One way to avoid this risk either not to store sensitive data in database or encrypt sensitive data columns or encrypt the file system itself.</p>

<p>Other way is to implement Data encryption at rest.</p>

<h3>Implement Data Encryption at rest, MySQL</h3>

<p>First step to create a key using <code>openssl</code> command</p>

<pre><code>openssl enc -aes-256-cbc -md sha1 -k &lt;enc password or passphase&gt; -in keys.txt -out mysql.enc
</code></pre>

<p>Second, edit <code>/etc/my.cnf</code> and provide encryption configurations</p>

<pre><code>plugin-load-add=file_key_management.so
file-key-management-filename=/var/lib/mysql/keys.txt
innodb-encrypt-tables
innodb-encrypt-log
encrypt-binlog
</code></pre>

<p><code>plugin-load-add</code> option will load the encryption plugin on mysql start.<br/>
<code>file-key-management-filename</code> option to provide reference to encryption key file.<br/>
<code>innodb-encrypt-tables</code> option will encrypt tables by default.<br/>
<code>innodb-encrypt-log</code> option will encrypt redo logs by default.<br/>
<code>encrypt-binlog</code> option will encrypt binlogs.</p>

<p>Third, restart mysql</p>

<pre><code>service mysql restart
</code></pre>

<p>Fourth, enable the same options on the replication slaves</p>

<p>Fifth, verify tables are encrypted using below mysql query</p>

<pre><code>select * from information_schema.INNODB_TABLESPACES_ENCRYPTION where ENCRYPTION_SCHEME=1;
</code></pre>

<br/>


<h3>innodb-encrypt-tables modes</h3>

<p>Three ways to encrypt tables<br/></p>

<ol>
<li><code>innodb-encrypt-tables</code> option in <code>my.cnf</code> will encrypt all tables unless table creation statement provided &ldquo;encrypted=no&rdquo; <br/></li>
<li><code>innodb-encrypt-tables</code> option in <code>my.cnf</code> will encrypt all tables and will not allow table creation with &ldquo;encrypted=no&rdquo; <br/></li>
<li><code>innodb-encrypt-tables</code> option is not provided in <code>my.cnf</code> but the plugin presents, it can explicitely encrypt a table by including &ldquo;encrypted=yes&rdquo; in table creation statement.<br/></li>
</ol>


<h3>Performance Overhead</h3>

<p>~5% to ~10% performance overhead observed in previous MySQL deployments with database enforced encryption mode.</p>

<p>Database-level encryption and selective tables encryption can be a better option than the filesystem-level encryption.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dive into Docker Image Layers]]></title>
    <link href="http://vishnuatrai.in/blog/2019/12/26/dive-into-docker-image-layers/"/>
    <updated>2019-12-26T20:48:58+05:30</updated>
    <id>http://vishnuatrai.in/blog/2019/12/26/dive-into-docker-image-layers</id>
    <content type="html"><![CDATA[<p>I recently used a tool named <a href="https://github.com/wagoodman/dive"><code>dive</code></a> to explore docker images developed and maintained by teammates or opensourced docker images. I used to look into <code>Dockerfile</code> steps to explore image layes. <code>dive</code> brings cli-ui and using arrow and tab keys we can explore each layer in left pane while layer contents listed in right pane.<!--more--></p>

<p>Essentially <code>dive</code> based on docker api and its cache contents and provides commands to explore layer contents and minimize the size of images.</p>

<p>Installation</p>

<p><code>brew install dive</code></p>

<p>command to explore a image layers</p>

<p><code>dive &lt;docker-image-id&gt;</code></p>

<p>to build a image using <code>dive</code> command</p>

<p><code>dive build -t &lt;image-tag&gt; .</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[jMeter - Optimize for load testing]]></title>
    <link href="http://vishnuatrai.in/blog/2019/11/11/jmeter-optimize-for-load-testing/"/>
    <updated>2019-11-11T17:40:57+05:30</updated>
    <id>http://vishnuatrai.in/blog/2019/11/11/jmeter-optimize-for-load-testing</id>
    <content type="html"><![CDATA[<p>These are the jMeter optimization steps we have taken to simulate 1M request per second concurrency test for REST application deployed on Kubernetes cluster.</p>

<h3>Use a High Capacity Jenkins Slave to run jMeter jmx script</h3>

<p>We have added a high capacity Jenkins slave node to run only jMeter performance test and have done jMeter installation on that server. All performance test jobs are restricted to run on that labeled slave node.<!--more-->
We can use available memory and CPU to optimize jMeter configuration.</p>

<h3>Run jMeter in non-GUI Mode</h3>

<p>Apart from developing jmx script or debugging request/response not use GUI mode to run load test. In GUI mode AWT Event Thread will disrupt both your test and JMeter in case of more or less high load. GUI mode consumes a lot of memory and other resources, which in turn negatively impacts your scripts and tests. Using the non-GUI mode of JMeter helps to reduce both resource requirements and potential errors.</p>

<h3>Increase the Java Heap Size</h3>

<p>Run JMeter with higher value of memory.</p>

<pre><code>`JVM_ARGS="-Xms512m -Xmx512m" jmeter.sh`
</code></pre>

<h3>Avoid Listeners</h3>

<p>Avoid UI listeners like graphs or results table to avoid OutOfMemory issues. Preferably only write results to a JTL.</p>

<h3>Minimize Metrics Need to Store</h3>

<p>Configure JMeter to ensure that it will only save the metrics that you absolutely need. You can control what to store by adding relevant lines to the user.properties file in jMeter installation.</p>

<h3>Generate Reports AFTER the Run</h3>

<p>It takes resources to be written (CPU and memory) and for analysis in XML format. Use the outputted .jtl files to create reports once the load test is finished. Building the report requires a great amount of CPU and memory resources.</p>

<h3>Tweak the JVM</h3>

<p>Settings like garbage collector (-XX:+UseConcMarkSweepGC), server JVM (-server) can be set inside JVM_ARGS by editing the JMeter launcher script.</p>

<p><code>NEW="-XX:NewSize=128m -XX:MaxNewSize=128m"</code> line in the JMeter command script should match with values provided in HEAP.</p>

<p><code>-XX:+UseConcMarkSweepGC</code> &ndash; this forces the usage of the CMS garbage collector. It will lower the overall throughput but leads to much shorter CPU intensive garbage collections.</p>

<p><code>-server</code> &ndash; this switches JVM into “server” mode with runtime parameters optimization. In this mode JMeter starts more slowly, but your overall throughput will be higher</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[React.js - Ways to Bind Events]]></title>
    <link href="http://vishnuatrai.in/blog/2019/10/12/react-dot-js-ways-to-bind-events/"/>
    <updated>2019-10-12T11:47:20+05:30</updated>
    <id>http://vishnuatrai.in/blog/2019/10/12/react-dot-js-ways-to-bind-events</id>
    <content type="html"><![CDATA[<h3>Background</h3>

<p>Event binding in ReactJs components required because of <code>this</code> keyword works in JavaScript, within click handler function <code>this</code> keyword will lost its context (component instance) or value.</p>

<h3>1. <code>bind</code> the handler in JSX render<!--more--></h3>

<pre><code>class BindHandler extends React.Component {
        constructor(props){
            ...
        }
        handlerFunction() { 
            console.log(this)
        }
        render(){
            return(&lt;button onClick={this.handlerFunction.bind(this)}&gt;Click Event&lt;/button&gt;)
        }
    }
</code></pre>

<h3>2. Arrow function in JSX render callback(<code>this</code> is bound lexically)</h3>

<pre><code>class ArrowFunctionBinding extends React.Component {
        constructor(props){
            ...
        }
        handlerFunction() { 
            console.log(this)
        }
        render(){
            return(&lt;button onClick={() =&gt; this.handlerFunction()}&gt;Click Event&lt;/button&gt;)
        }
    }
</code></pre>

<h3>3. <code>bind</code> in <code>constructor()</code></h3>

<pre><code>class ConstructorBinding extends React.Component {
        constructor(props){
            ...
            this.handlerFunction = this.handlerFunction.bind(this)
        }
        handlerFunction() { 
            console.log(this)
        }
        render(){
            return(&lt;button onClick={this.handlerFunction}&gt;Click Event&lt;/button&gt;)
        }
    }
</code></pre>

<h3>4. Define handler function as class property using Arrow function</h3>

<pre><code>class EventHandlerClassProperty extends React.Component {
        constructor(props){
            ...
            this.handlerFunction = this.handlerFunction.bind(this)
        }
        handlerFunction = () =&gt; { 
            console.log(this)
        }
        render(){
            return(&lt;button onClick={this.handlerFunction}&gt;Click Event&lt;/button&gt;)
        }
    }
</code></pre>

<h3>Recommendations</h3>

<p>React documentation suggests binding events in constructor. But if need to use <strong>arrow function in JSX render callback</strong> for its simplicity or when need to pass arguments, we should consider caching the handlers if the bindings become a performance issue.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is new in Rails 6]]></title>
    <link href="http://vishnuatrai.in/blog/2019/09/15/what-is-new-in-rails-6/"/>
    <updated>2019-09-15T14:09:12+05:30</updated>
    <id>http://vishnuatrai.in/blog/2019/09/15/what-is-new-in-rails-6</id>
    <content type="html"><![CDATA[<h4>Rails 6 require Ruby version 2.5 or greater. Upgrade to at least rails 5.2 or later versions and make sure application run properly without any failure, then attempt to upgrade to 6.0, follow <a href="https://edgeguides.rubyonrails.org/upgrading_ruby_on_rails.html#upgrading-from-rails-5-2-to-rails-6-0">upgrade guide</a>.</h4>

<h3>ApplicationRecord &ndash; Multiple databases support</h3>

<p>Active Record supports switching between multiple databases with a minimal change in codebase but a major impact on multiple replica set db. With multiple db support, rails application can have a read-only version of your database to use in areas known for having slow queries, or if it need to write to different databases depending on which controller request.
This requires a change in <code>database.yml</code> setup, ie. <!--more-->
`</p>

<pre><code>development:
  primary:
    database: primary_db
    user: rw_user
  primary_replica:
    database: primary_db
    user: ro_user
    replica: true
  animals:
    database: animals_db
    user: rw_user
  animals_replica
    database: animals_db
    user: ro_user
    replica: true
</code></pre>

<p>`
You can then specify at the model-level which database(s) you want to use:</p>

<pre><code>class Animal &lt; ApplicationRecord
    connects_to database: { writing: :animals, reading: :animals_replica }
end
</code></pre>

<p>And then it’s just one line of code to temporarily switch between databases inside a block!</p>

<pre><code>Aminal.connected_to(role: :reading) do
    #Slow queries in jobs can be performaed here
end
</code></pre>

<h3>Action Mailbox</h3>

<p>ActionMailbox provides a set of tools that will allow applications to route incoming mail into controller-like mailboxes for processing. Action Mailbox requires Active Job and Active Storage as part of it’s and a database table <code>InboundEmail</code>.</p>

<h4>Setup</h4>

<pre><code>bin/rails action_mailbox:install
bin/rails db:migrate
</code></pre>

<h4>Routing and Processing</h4>

<pre><code># app/mailboxes/application_mailbox.rb
class ApplicationMailbox &lt; ActionMailbox::Base
    routing /^comment\+(.+)@example\.com/i =&gt; :comments
end
# app/mailboxes/comments_mailbox.rb
class CommentsMailbox &lt; ApplicationMailbox
    def process
        user = User.find_by(email: mail.from)
        post_uuid = COMMENTS_REGEX.match(mail.to)[1]

        post = Post.find_by(uuid: post_uuid)
        post.comments.create(user: user, content: mail.body)
    end
end
</code></pre>

<h3>Action Text</h3>

<p>ActionText is am implementation of rich-text support(Trix editor by Basecamp). Run bin/rails action_text:install to add the Yarn package and copy over the necessary migration. Also, you need to set up Active Storage for embedded images and other attachments. Both trix and @rails/actiontext should be required in your JavaScript pack.</p>

<pre><code>// application.js
require("trix")
require("@rails/actiontext")
</code></pre>

<p>It exposes a has_rich_text method that we can apply to models and Action Text will take care of the rest.</p>

<pre><code># app/models/article.rb
class Article &lt; ApplicationRecord
    has_rich_text :content
end
</code></pre>

<h3>Parallel Testing</h3>

<p>Rails 6 adds parallel test to Rails applications by default. It parallelize test suite and enable faster test suite run times and efficiency. Configuration via parent test class</p>

<pre><code>class ActiveSupport::TestCase
    parallelize(workers: 2, with: :processes) # or :threads
end
</code></pre>

<p>or through environment variable PARALLEL_WORKERS and it’ll create the database with a numbered suffix.</p>

<pre><code>PARALLEL_WORKERS=2 rails test
</code></pre>

<h3>Action Cable Testing</h3>

<p>Action Cable testing tools allow you to test your Action Cable functionality at any level: connections, channels, broadcasts. By default, when you generate new Rails application with Action Cable, a test for the base connection class (<code>ApplicationCable::Connection</code>) is generated as well under <code>test/channels/application_cable</code> directory.</p>

<h3>Webpack</h3>

<p><code>webpacker</code> gem, replacing the previously-default Rails Asset Pipeline and providing for better interoperability with modern JavaScript libraries and coding standards. Using webpacker gem, all StyleSheets, images and JS libraries wrap into a single bundle with a single access point.</p>

<h3>Zeitwerk</h3>

<p>Zeitwerk is a new and improved, thread-safe code loader for Rails, Configuration to enable &ndash;</p>

<pre><code>config.autoloader = :zeitwerk
</code></pre>

<p><a href="https://edgeguides.rubyonrails.org/6_0_release_notes.html">https://edgeguides.rubyonrails.org/6_0_release_notes.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Jenkins Slave with Docker Executors]]></title>
    <link href="http://vishnuatrai.in/blog/2019/08/08/jenkins-slave-with-docker-executors/"/>
    <updated>2019-08-08T16:35:25+05:30</updated>
    <id>http://vishnuatrai.in/blog/2019/08/08/jenkins-slave-with-docker-executors</id>
    <content type="html"><![CDATA[<p><a href="https://wiki.jenkins.io/display/JENKINS/Docker+Plugin">Docker Plugin</a> enables jenkins to run jobs as docker container. In such case we dont need to setup jenkins nodes(agents) with specific binaries, instead docker images will be used to run jobs.<!--more-->Jenkins master needs to be configured with docker host where we can push docker images, those will be used by agents to execute jobs.</p>

<h3>Global Configuration</h3>

<p>After installation of <code>Docker Plugin</code> from manage plugins options, we will need to configure docker host and templates to create docker executers in Global Configuration option.</p>

<p>Go to <strong>Global Configurations</strong> &ndash;> <strong>Docker</strong> &ndash;> fill the below configuration options to setup docker host <br />
<strong>Name</strong>    Name of docker slave to be used in job configuration  <br />
<strong>Docker Url</strong>    docker engine url and port  <br /></p>

<p><img src="http://vishnuatrai.in/images//posts/Jenkins7.png" alt="" /></p>

<p><strong>Docker Template</strong>    docker template is an executor, we can add multiple templates and it will enable multiple executors  <br />
Fill below configurations to setup docker template <br /></p>

<ul>
<li><strong>Docker Image</strong> image name that that is available on docker host and having binaries to run the job</li>
<li><strong>Container Settings</strong> > <strong>Volumes</strong> Provide the container volumes(ie. <code>/home/dockerslave/bin</code>) where the binaries available, these are mapped with host machine and binaries will be available on host machine to run the job.</li>
<li><strong>Label</strong> Provide the label to uniquely identify the executor and configure in job configuration</li>
<li><strong>Launch Method</strong> select <strong>Docker ssh compute launcher</strong></li>
<li><strong>Credentials</strong> select credential id to ssh into docker host container</li>
</ul>


<p><img src="http://vishnuatrai.in/images//posts/Jenkins4.png" alt="" />
<img src="http://vishnuatrai.in/images//posts/Jenkins5.png" alt="" />
<img src="http://vishnuatrai.in/images//posts/Jenkins6.png" alt="" /></p>

<h3>Job Configuration</h3>

<p><strong>Build</strong> > check <strong>Restrict where this project can be run</strong> > <strong>Label Expression</strong> > provide the docker template lable</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kafka - Consumer CommitFailedException for revoked partition]]></title>
    <link href="http://vishnuatrai.in/blog/2019/06/10/kafka-consumer-commitfailedexception-for-revoked-partition/"/>
    <updated>2019-06-10T16:30:34+05:30</updated>
    <id>http://vishnuatrai.in/blog/2019/06/10/kafka-consumer-commitfailedexception-for-revoked-partition</id>
    <content type="html"><![CDATA[<h4>Error Trace</h4>

<pre><code>06-10 01:15:05 131 pool-9-thread-5 ERROR [] -
commit failed
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.sendOffsetCommitRequest(ConsumerCoordinator.java:713) ~[MsgAgent-jar-with-dependencies.jar:na]
at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:596) ~[MsgAgent-jar-with-dependencies.jar:na]
at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1218) ~[ConsumerService-jar-with-dependencies.jar:na]
at com.vatrai.consumer.service.ConsumerServiceImpl.consume(ConsumerServiceImpl.java:67) ~[ConsumerService-jar-with-dependencies.jar:na]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_161]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_161]
at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161]
</code></pre>

<h4>Reason</h4>

<p>Consumer invoked the polling method to obtain the messages.<!--more--> Client application uses an external loop to continuously invoke the polling method of the consumer. The next poll was performed only after all messages in a batch were processed. <code>max.poll.interval.ms</code> option can be configured to set maximum interval between two polling operations. If client application cannot process polling records and it takes more time than the maximum interval. As a result, the server may remove the client and trigger rebalance.</p>

<h4>Optimize Configuration</h4>

<p><strong><em>max.poll.interval.ms=75000</em></strong></p>

<p>The maximum delay between invocations of poll() when using consumer group management. This places an upper bound on the amount of time that the consumer can be idle before fetching more records. If poll() is not called before expiration of this timeout, then the consumer is considered failed and the group will rebalance in order to reassign the partitions to another member. For consumers using a non-null group.instance.id which reach this timeout, partitions will not be immediately reassigned. Instead, the consumer will stop sending heartbeats and partitions will be reassigned after expiration of session.timeout.ms. This mirrors the behaviour of a static consumer which has shutdown.</p>

<p><strong><em>max.poll.records = 300</em></strong></p>

<p>The maximum number of records returned in a single call to poll(). Number of polling records can be reduced if processing these records taking more time.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Jenkins - Add Slave nodes as JNLP agents]]></title>
    <link href="http://vishnuatrai.in/blog/2019/04/15/jenkins-add-slave-nodes-as-jnlp-agents/"/>
    <updated>2019-04-15T15:36:54+05:30</updated>
    <id>http://vishnuatrai.in/blog/2019/04/15/jenkins-add-slave-nodes-as-jnlp-agents</id>
    <content type="html"><![CDATA[<p>If it is required to run Jenkins master in an isolated network and master should not be allowed to connected to its nodes(agents), we can use JNLP method to add agents to master to process jobs in distributed manner. In this scenario, its not desirable to have master connections with slave(agent) nodes but agent to master connections only required. <!--more--></p>

<h3>Only Agent to master connections</h3>

<p>In this case the agent node will not be visible to the master, so the master can not initiate the agent process. You can use a different type of agent configuration in this case called &ldquo;JNLP&rdquo;. This means that the master does not need network &ldquo;ingress&rdquo; to the agent (but the agent will need to be able to connect back to the master). Handy for if the agents are behind a firewall, or perhaps in some more secure environment to do trusted deploys.</p>

<h3>Configuration</h3>

<p>In order to setup a slave agent in above scenario you need to first <strong>Enable the JNLP Agents</strong>:</p>

<p>Go to <strong>Manage Jenkins</strong> &ndash;> <strong>Configure Global Security</strong> &ndash;> under <strong>Agents</strong> section &ndash;> <strong>TCP port for inbound agents</strong> &ndash;> select <strong>Random</strong> &ndash;><strong>Save</strong>.</p>

<p><img src="http://vishnuatrai.in/images//posts/Jenkins1.png" alt="" /></p>

<h4>Setup slave agent node</h4>

<p>Go to <strong>Manage Jenkins</strong> &ndash;> <strong>Manage Nodes</strong> &ndash;>click on <strong>New Node</strong> &ndash;> Enter the <strong>node name</strong> &ndash;> Select <strong>permanent agent</strong>.</p>

<p>Fill the below details to configure the slave agent</p>

<h4>Description</h4>

<h4>Remote root directory</h4>

<p>This should be the workspace directory on slave agent</p>

<h4>Label</h4>

<p>Provide the label to uniquely identify the slave node</p>

<h4>Launch Method</h4>

<p>Select <strong>launch agent by Connecting it to the master</strong> for windows agents  <br />
and <strong>launch agents via ssh</strong> for linux agents</p>

<p><img src="http://vishnuatrai.in/images//posts/Jenkins2.png" alt="" /></p>

<p>To Launch the slave agent via command line  <br />
<strong>Download the agent.jar file and copy to agent node</strong>  <br />
<strong>Run agent.jar using command line</strong></p>

<pre><code>java --jar agent.jar -jnlpUrl &lt;jenkins master url&gt; -secret &lt;secret given on node configuration&gt; -workDir "/user/agent/home/workspace"
</code></pre>

<p><img src="http://vishnuatrai.in/images//posts/Jenkins3.png" alt="" /></p>

<p>With above command agent is authorized and registered with jenkins master. Now agent is successfully configured and launched, which can be verified on master nodes. Jenkins master can delegate jobs to agent node.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MongoDB 4.0 - Support for ACID transactions]]></title>
    <link href="http://vishnuatrai.in/blog/2018/12/20/mongodb-4-dot-0-support-for-acid-transactions/"/>
    <updated>2018-12-20T17:41:34+05:30</updated>
    <id>http://vishnuatrai.in/blog/2018/12/20/mongodb-4-dot-0-support-for-acid-transactions</id>
    <content type="html"><![CDATA[<h3>Support for Single-Shard multi document ACID transactions</h3>

<p>To support benefits of ACID transactions with fully-denormalized document data modeling, MongoDB added single-shard ACID transaction support in the released 4.0. These single-shard transactions apply ACID properties on updates across multiple documents those are present in the same shard. Multi-shard transactions supposed to implement in release 4.2.</p>

<h4>Scenario, how it works?</h4>

<ol>
<li>Client application will get a database session <!--more--></li>
<li><p>Database session initiate transaction block using <code>start_transaction</code> statement with <code>majority</code> <code>writeConcern</code> in <code>try</code> block. The only <code>writeConcern</code> thats suitable for high data durability is that of majority. This means a majority of replicas should commit the changes before the primary acknowledges the success of the write to the client. The transaction will remain blocked till at least 1 of the 2 secondaries pulls the update from the primary using asynchronous replication which is susceptible to unpredictable replication lag especially under heavy load.</p></li>
<li><p>Within transaction block multiple insert/update/remove statements can be made.</p></li>
<li>At end of transaction block <code>commit_transaction</code> required to commit data in database.</li>
<li>If any exception occurs in transaction block, client application should <code>abort_transaction</code> in <code>catch</code> block to roll back updates.</li>
<li>Finally the database client session can be closed in <code>finally</code> block</li>
</ol>


<p>Sample Java code to mimic same scenario,</p>

<pre><code>###Java sample
private void multiDocumentTransactions() {
    ClientSession session = client.startSession();
    try {
        session.startTransaction(TransactionOptions.builder().writeConcern(WriteConcern.MAJORITY).build());
        orders.updateOne(session, {..filter..}, {..updates..});
        stock.updateOne(session, {..filter..}, {..updates..});
        session.commitTransaction();
    } catch (MongoCommandException e) {
        session.abortTransaction();
        //ROLLBACK TRANSACTION
    } finally {
        session.close();
    }
}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ProxySQL - MySQL High Availability Load Balancing]]></title>
    <link href="http://vishnuatrai.in/blog/2018/11/12/proxysql-mysql-high-availability-load-balancing/"/>
    <updated>2018-11-12T22:59:40+05:30</updated>
    <id>http://vishnuatrai.in/blog/2018/11/12/proxysql-mysql-high-availability-load-balancing</id>
    <content type="html"><![CDATA[<h3>What is ProxySQL?</h3>

<ul>
<li>Lightweight Proxy layer for MySQL</li>
<li>Its protocol aware that we put between application and database server</li>
<li>Improve database operations <!--more--></li>
<li>Understand and solve performance issues</li>
<li>A proxy layer to shield the database</li>
<li>Add high availability to database topology</li>
</ul>


<h3>Why ProxySQL/HAProxy?</h3>

<p>ProxySQL provides below advantages if included in deployment stack</p>

<ul>
<li>Scalability <br />
  Connection Pooling and multiplexing,  Read/Write split,  Read/Write sharding</li>
<li>High Availability <br />
  Seamless failover,  Load balancing,  Cluster aware</li>
<li>Advance query and support <br />
  Query caching,  Query rewrite,  Query blocking,  Query mirroring,  Query throttling,  Query timeout</li>
<li>Manageability <br />
  Admin Utility,  Runtime reconfiguration,  Monitoring</li>
</ul>


<h3>ProxySQL Configurations Options</h3>

<pre><code>mysql_replication_hostgroups
mysql_servers
mysql_server_connect_log
mysql_server_ping_log
mysql_server_replication_lag_log
mysql_server_read_only_log
stats_mysql_connection_pool
stats_mysql_commands_counters
mysql_query_rules
stats_mysql_query_digest
</code></pre>

<h3>Deploying with Kubernetes/Openshift &ndash; The Sidecar Pattern</h3>

<p>We can deploy the <code>proxysql</code> image in same pod as application image but data base references we will need to provide as <code>localhost</code> or <code>127.0.0.1</code> and port will be <code>6033</code> (the default proxysql port). The application will connect to proxysql server and it will then redirect database queries to mysql database cluster.</p>

<p>Openshift/kubernetes template spec &ndash;</p>

<pre><code>spec:
 template:
  spce:
   volumes:
    -
   name: "proxysql-configiguration-file"
     secret:
      secretName: "proxysql-configuration"
   containers:
    -
   name: "proxysql-service"
     image: "docker.com/proxysql/proxysql:1.0"
     ports:
      -
   containerPort: 6033
       protocal: TCP
   env:
   resources:
   volumeMounts:
    -
   name: "proxysql-configuration"
     readOnly: true
     mountPath: "/etc/prosysql-config"
</code></pre>

<h3>References</h3>

<p><a href="https://www.proxysql.com/">https://www.proxysql.com/</a></p>

<p>docker sandbox</p>

<p><a href="https://github.com/vishnuatrai/MySQLSandbox">https://github.com/vishnuatrai/MySQLSandbox</a></p>

<p>github</p>

<p><a href="https://github.com/sysown/proxysql">https://github.com/sysown/proxysql</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubernetes - Secrets vs ConfigMaps]]></title>
    <link href="http://vishnuatrai.in/blog/2018/08/16/kubernetes-secrets-vs-configmaps/"/>
    <updated>2018-08-16T16:28:34+05:30</updated>
    <id>http://vishnuatrai.in/blog/2018/08/16/kubernetes-secrets-vs-configmaps</id>
    <content type="html"><![CDATA[<!--more-->




<table class="kubtable">
    <tr style="font-weight: bold;">
        <th style="width: 16%"></th>
        <th style="width: 42%"> <strong>Secret</strong></th>
        <th style="width: 42%"> <strong>ConfigMap</strong></th>
    </tr>
    <tbody>
        <tr>
        <td><strong>What is?</strong></td>
        <td>Secrets are k8s object to manage small amount of sensitive data like password, keys and tokens with less than 1mb size. Secrets encoded and stored inside k8s master etcd data store. Since Secrets will be created outside of pods and containers, these can be used any number of times</td>
        <td>ConfigMaps used to seperate container images and its custom configurations so that images are portable and can be run in any environment providing appropriate configuration. `ConfigMap` stores data in key, value format. If any configuration values are sensitive the use `Secret` instead `ConfigMap`. Its must to create `ConfigMap` before hand if we need to refer in pod spec</td>
        </tr>
        <tr>
        <td><strong class="feature">Create using `kubectl`</strong></td>
        <td >
            <span class="syntax">   
                #syntax <br/>
                kubectl create secret <type of secret (generic)> <name of secret> < data from-file|from-literal><br/>
                echo -n &#8216;admin&#8217; > ./username.txt<br/>
                echo -n &#8216;1f2d1e2e67df&#8217; > ./password.txt<br/>
                kubectl create secret generic db-user-pass &#8211;from-file=./username.txt &#8211;from-file=./password.txt<br/>
                kubectl create secret generic dev-db-secret &#8211;from-literal=username=user &#8211;from-literal=password=&#8217;S!B\*d$zDsb=&#8217;
            </span>
        </td>
        <td>
            <span  class="syntax">
                #syntax<br/>
                kubectl create configmap <name of configmap> < data from-file|from-literal><br/>
                path/to/config/file/application.yaml<br/>
                path/to/config/file/application-prod.properties<br/>
                kubectl create configmap app-config &#8211;from-file=path/to/config/file/<br/>
                #list the content of properties and yml file configuration<br/>
                kubectl get configmaps app-config -o wide<br/>
                kubectl create configmap custom-config &#8211;from-literal port=8080 &#8211;from-literal https=false
            </span>
        </td>
        </tr>
        <tr>
            <td><strong class="feature">Create using yaml manifesto files</strong></td>
            <td>
                <span  class="syntax">
                <pre>
                    <code>
    #sample yaml manifesto secret.yaml file 
    apiVersion: v1
    kind: Secret
    metadata:
        name: mysecret
    type: Opaque
    data:
        username: appuser
        password: MWYyZDFlMmU2N2Rm
&nbsp;
&nbsp;
create using kubectl apply
    kubectl apply -f ./secret.yaml
                </span>
            </td>
            <td>
            <span  class="syntax">
                <pre>
                <code>
    #sample yaml manifesto configmap.yaml file 
    apiVersion: v1
    kind: ConfigMap
    metadata:
        name: myconfig
    type: Opaque
    data:
        port: 8080
        https: false
&nbsp;
&nbsp;
&nbsp;
create using kubectl apply
    kubectl apply -f ./configmap.yaml
                </code>     
                </pre>      
            </span>
            </td>
        </tr>
        <tr>
            <td>
                <strong class="feature">
                    Deploy within Pods, 
                    Using Volumes
                </strong>
            </td>
            <td>
                <span  class="syntax">
                <pre>
                <code>
                #sample pod manifesto yaml file
                apiVersion: v1
                kind: Pod
                metadata:
                    name: nginx
                spec:
                    containers:
                    - name: nginx
                      image: nginx
                      volumeMounts:
                      - name: secret-volume
                        mountPath: /etc/secret
                        readOnly: true
                    volumes:
                    - name: secret-volume
                      secret:
                        secretName: mysecret    
                </code>     
                </pre>      
                </span>
            </td>
            <td>
                <span  class="syntax">
                <pre>
                <code>
    #sample pod manifesto yaml file
    apiVersion: v1
    kind: Pod
    metadata:
        name: nginx
    spec:
        containers:
        - name: nginx
          image: nginx
          volumeMounts:
          - mountPath: /target/
            name: custom-config
        volumes:
        - name: custom-config
          configMap:
            name: app-config
            items:
            - key: application.yaml  
              path: application.yaml
            - key: application-prod.properties
              path: application-prod.properties
                </code>
                </pre>
                </span>
            </td>
        </tr>
        <tr>
            <td>
                <strong class="feature">
                    Deploy within Pods, 
                    Using environment variables
                </strong>
            </td>
            <td>
                <span class="syntax">
                    <pre><code>
    #sample pod manifesto yaml file
    apiVersion: v1
    kind: Pod
    metadata:
        name: nginx
    spec:
        containers:
        - name: nginx
          image: nginx
          env:
          - name: SECRET_USERNAME
            valueFrom:
                secretKeyRef:
                    name: mysecret
                    key: username
          - name: SECRET_PASSWORD
            valueFrom:
                secretKeyRef:
                    name: mysecret
                    key: password


Instead of volume mapping use environment variables
and use secret(ie. mysecret) to assign values. 
Verify the environment variables.
&nbsp;
    kubectl exec nginx env | grep SECRET
        => SECRET_PASSWORD=MWYyZDFlMmU2N2Rm
        => SECRET_USERNAME=appuser
                    </code></pre>
                </span>
            </td>
            <td>
                <span class="syntax"><pre><code>
    #sample pod manifesto yaml file
    apiVersion: v1
    kind: Pod
    metadata:
        name: nginx
    spec:
        containers:
        - name: nginx
          image: nginx
          env:
          - name: CONFIG_PORT
            valueFrom:
                configMapKeyRef:
                    name: myconfig
                    key: port


Instead of volume mapping use environment variables
and use configmap(ie. myconfig) to assign values.
Verify the environment variables.
&nbsp;
    kubectl exec nginx env | grep CONFIG
        => CONFIG_PORT=8080             
                </code></pre></span>
            </td>
        </tr>
    </tbody>
</table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MongoDB Change Streams - Real time oplog notifications]]></title>
    <link href="http://vishnuatrai.in/blog/2018/07/05/mongodb-change-streams-real-time-notifications-to-subscribers-for-data-updates/"/>
    <updated>2018-07-05T17:19:31+05:30</updated>
    <id>http://vishnuatrai.in/blog/2018/07/05/mongodb-change-streams-real-time-notifications-to-subscribers-for-data-updates</id>
    <content type="html"><![CDATA[<p>Change Stream is easiest way to subscribe database changes realtime, its based on mongoDB oplog(Operation Log) technology. An ideal use case would be ETL operational data to a reporting and visualization data store via kafka data pipeline.</p>

<h3>MongoDB real time data sync technologies</h3>

<ol>
<li>mongodb oplog technology based changestream enable applications to stream real-time data changes<!--more--></li>
<li>changestream can notify your application of all writes to documents (including deletes) without polling.</li>
<li>changestream advantages over oplog</li>
</ol>


<h2>Use case</h2>

<p>MongoDB changestream (operation logs, create, update, delete) will be as data source. Java springboot based application to subscribe changes and push to kafka topic. Kafka data pipeline can refine data, transform, filter and sync in to a big data storage for reporting, visualization or many other purpose.</p>

<pre><code>###Java sample
import com.mongodb.Block;
import com.mongodb.MongoClient;
import com.mongodb.MongoClientURI;
import com.mongodb.client.MongoCollection;
import com.mongodb.client.MongoDatabase;
import com.mongodb.client.model.changestream.FullDocument;
import com.mongodb.client.model.changestream.ChangeStreamDocument;
import org.bson.Document;


MongoClient mongoClient = new MongoClient(new MongoClientURI("mongodb://localhost:27017,localhost:27018,localhost:27019"));
MongoDatabase database = mongoClient.getDatabase("test");
MongoCollection&lt;Document&gt; collection = database.getCollection("restaurants");

Block&lt;ChangeStreamDocument&lt;Document&gt;&gt; printBlock = new Block&lt;&gt;() {
@Override
    public void apply(final ChangeStreamDocument&lt;Document&gt; changeStreamDocument) {
        System.out.println(changeStreamDocument);
    }
};

collection.watch().forEach(printBlock);
</code></pre>

<h3>changestream advantages over oplog</h3>

<ol>
<li><strong><em>Use access control</em></strong> (no admin user required to access changestream)</li>
<li><strong><em>Present a Defined API</em></strong>, API syntax takes advantage of the established MongoDB query language, and are independent of the underlying oplog format.</li>
<li><strong><em>Total Ordering</em></strong>, subscriber applications will always receive changes in the order they were applied to the database.</li>
<li><strong><em>Filters</em></strong>, Changes can be filtered to provide relevant and targeted changes to listening applications. As an example, filters can be on operation type or fields within the document.</li>
<li><strong><em>Power of aggregation</em></strong>, define change streams on any collection just like any other normal aggregation operators using <code>$changeStream</code> operator and <code>watch()</code> method</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is new in Rails 5.2]]></title>
    <link href="http://vishnuatrai.in/blog/2018/01/20/what-is-new-in-rails-5-dot-2/"/>
    <updated>2018-01-20T16:02:51+05:30</updated>
    <id>http://vishnuatrai.in/blog/2018/01/20/what-is-new-in-rails-5-dot-2</id>
    <content type="html"><![CDATA[<h3>Active Storage</h3>

<p>Active Storage supports modern approach for file uploading to Amazon S3, Google Cloud Storage, Microsoft Azure Cloud file storage. It will also provide references to active record database tables <code>active_storage_blobs</code> and <code>active_storage_attachments</code>. <code>rails active_storage:install</code> will install initial setup for active storage.<!--more-->Configure and setup <code>config/storage.yml</code> cloud credentials and storage buckets.</p>

<pre><code>#config/storage.yml
local:
  service: Disk
  root: &lt;%= Rails.root.join("storage") %&gt;
test:
  service: Disk
  root: &lt;%= Rails.root.join("tmp/storage") %&gt;
amazon:
  service: S3
  access_key_id: ""
  secret_access_key: ""
  bucket: ""
  region: ""
</code></pre>

<h3>Redis Cache Store</h3>

<p>Rails 5.2 ships with built-in Redis cache store. The Redis cache store takes advantage of Redis support for automatic eviction when it reaches max memory, allowing it to behave much like a Memcached cache server.</p>

<p>Finally, add the configuration in the relevant <code>config/environments/*.rb</code> file:</p>

<pre><code>config.cache_store = :redis_cache_store, { url: 'redis://redis-server:6379' }
</code></pre>

<h3>HTTP/2 Early Hints</h3>

<p>This means we can automatically instruct the web server to send required style sheet and JavaScript assets early. Which means faster full page delivery.</p>

<p>To start the server with Early Hints enabled pass <code>--early-hints</code> to <code>rails server</code></p>

<h3>Credentials</h3>

<p>Added <code>config/credentials.yml.enc</code> file to store production app secrets. It allows saving any authentication credentials for third-party services directly in repository encrypted with a key in the <code>config/master.key</code> file or the <code>RAILS_MASTER_KEY</code> environment variable</p>

<p>To add new secret to credentials, first run rails secret to get a new secret. Then run rails credentials:edit to edit credentials, and add the secret. Running credentials:edit creates new credentials file and master key, if they did not already exist.</p>

<p>By default, this file contains the application&rsquo;s <code>secret_key_base</code>, but it could also be used to store other credentials such as access keys for external APIs.</p>

<p>The secrets kept in credentials file are accessible via <code>Rails.application.credentials</code>. For example, with the following decrypted <code>config/credentials.yml.enc</code></p>

<p>  #config/credentials.yml.enc</p>

<pre><code>secret_key_base:&lt;secret key base&gt;
api_client_key: &lt;secret key1&gt;
api_client_secret: &lt;secret key2&gt;
</code></pre>

<h3>Content Security Policy</h3>

<p>Content security policy can be configured as a global default policy and then override it on a per-resource basis and even use lambdas to inject per-request values into the header such as account subdomains in a multi-tenant application.</p>

<p>The HTTP <code>Content-Security-Policy</code> response header allows web site administrators to control resources the user agent is allowed to load for a given page. With a few exceptions, policies mostly involve specifying server origins and script endpoints.</p>

<p>Example &ndash;
  # config/initializers/content_security_policy.rb</p>

<pre><code>Rails.application.config.content_security_policy do |policy|
  policy.default_src :self, :https
  policy.font_src    :self, :https, :data
  policy.img_src     :self, :https, :data
  policy.object_src  :none
  policy.script_src  :self, :https
  policy.style_src   :self, :https

  # Specify URI for violation reports
  policy.report_uri "/csp-violation-report-path"
end
</code></pre>

<p>Example controller overrides:</p>

<pre><code># Override policy inline
  class PostsController &lt; ApplicationController
    content_security_policy do |p|
      p.upgrade_insecure_requests true
    end
  end

# Using literal values
  class PostsController &lt; ApplicationController
    content_security_policy do |p|
      p.base_uri "https://www.example.com"
    end
  end

# Disabling the global CSP
  class PagesController &lt; ApplicationController
    content_security_policy false, only: :index
  end
</code></pre>

<br/>


<p>References: <a href="https://guides.rubyonrails.org/5_2_release_notes.html">https://guides.rubyonrails.org/5_2_release_notes.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch - mapper_parsing_exception, failed to parse field]]></title>
    <link href="http://vishnuatrai.in/blog/2017/12/20/elasticsearch-mapper-parsing-exception/"/>
    <updated>2017-12-20T15:37:59+05:30</updated>
    <id>http://vishnuatrai.in/blog/2017/12/20/elasticsearch-mapper-parsing-exception</id>
    <content type="html"><![CDATA[<p>We have recently faced issue with Elasticsearch field types and mapping, when if a field is mapped with a type other type can not be indexed for the same field. Elasticsearch indesing mechanism ristrict that. for example we have created a index named <code>users</code> and inserted a record with <code>text</code> type field <code>name</code>. <!--more--></p>

<p><img src="http://vishnuatrai.in/images//posts/elasticsearch1.png" alt="" /></p>

<p>Second record we want to insert same field with another type per say <code>object</code>, it will raise exceptation <code>mapper_parsing_exception, failed to parse field</code>.</p>

<p><img src="http://vishnuatrai.in/images//posts/Elasticsearch2.png" alt="" /></p>

<p>Because with insertation of first record the index will create a mapping and provide the field type <code>text</code> with second record insertation where the type is an object it will not match with the mapping and raise exceptation.</p>

<p><img src="http://vishnuatrai.in/images//posts/elasticsearch3.png" alt="" /></p>

<h3>Simple Hack But Many pitfalls</h3>

<p>Since by nature Elasticsearch doesn&rsquo;t support above scenario, once the mapping is created and type is assigned to a field it will not allow indexing of other type. However in my case we have to store the data in elasticsearch engine, so converted the object to string and that worked. But it will not support query on the converted object and on client side also we will need to convert back to object type.</p>

<p><img src="http://vishnuatrai.in/images//posts/elasticsearch4.png" alt="" /></p>

<p>After converting the object to text it will indexed as a text field only and will look like a escaped string object.</p>

<p><img src="http://vishnuatrai.in/images//posts/elasticsearch5.png" alt="" /></p>

<p>Still investigating on better solution or way to work on this issue.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySQL - Distributed GET_LOCK]]></title>
    <link href="http://vishnuatrai.in/blog/2017/10/15/mysql-distributed-GET_LOCK/"/>
    <updated>2017-10-15T17:52:24+05:30</updated>
    <id>http://vishnuatrai.in/blog/2017/10/15/mysql-distributed-GET_LOCK</id>
    <content type="html"><![CDATA[<p>Distributed global locking using <code>mysql get_lock()</code>, ensures releasing orphaned lock.</p>

<h3>USE CASE 1:</h3>

<h4>Eliminate SPOF of background jobs or scheduled/cron job</h4>

<h3>USE CASE 2:</h3>

<h4>A process that allowed to run only once on a given time however the process is deployed on multiple hosts as part of different micro-services, ie. scheduled jobs</h4>

<br />


<p><strong><em><code>GET_LOCK(str,timeout)</code></em></strong> <br /></p>

<p>Tries to obtain a lock with a name given by the string <code>str</code>, using a timeout of <code>timeout</code> seconds. A negative <code>timeout</code> value means infinite timeout. The lock is exclusive. While held by one session, other sessions cannot <!--more--> obtain a lock of the same name.</p>

<p>Returns <code>1</code> if the lock was obtained successfully, <code>0</code> if the attempt timed out (for example, because another client has previously locked the name), or <code>NULL</code> if an error occurred (such as running out of memory or the thread was killed with mysqladmin kill).</p>

<p>A lock obtained with <code>GET_LOCK()</code> is released explicitly by executing <code>RELEASE_LOCK()</code> or implicitly when your session terminates (either normally or abnormally). Locks obtained with <code>GET_LOCK()</code> are not released when transactions commit or roll back.</p>

<h4>Sample Java snippets</h4>

<pre><code>import java.sql.DriverManager;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
.....
Connection connection = DriverManager.getConnection("jdbc:mysql://localhost/" + dbName, "root", "");
PreparedStatement sth = connection.prepareStatement("SELECT GET_LOCK(?, ?)")
sth.setString(1, "distributed_lock");
sth.setInt(2, 2);

ResultSet resultSet = sth.executeQuery();
resultSet.next();
Integer status = resultSet.getInt(1);

if (resultSet.wasNull()) {
   throw new Exception("Can not obtain lock");
}
if (status == 0) {
  throw new Exception("Already locked `distributed_lock`");
}
if (status == 1) {
    //successfully get lock.. do something here ...
}
.....
</code></pre>

<p>Reference &ndash; <a href="https://dev.mysql.com/doc/refman/5.7/en/locking-functions.html#function_get-lock">MySQL GET_LOCK</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MongoDB Replica Set - localhost and Ops Manager]]></title>
    <link href="http://vishnuatrai.in/blog/2017/04/21/mongodb-replica-set-localhost-and-ops-manager/"/>
    <updated>2017-04-21T17:28:18+05:30</updated>
    <id>http://vishnuatrai.in/blog/2017/04/21/mongodb-replica-set-localhost-and-ops-manager</id>
    <content type="html"><![CDATA[<h3>Setup Local DevEnv Replica Set</h3>

<ol>
<li><p>update <code>mongod.conf</code> for replication</p>

<pre><code> #edit /usr/local/etc/mongod.conf
 systemLog:
     destination: file
     path: /usr/local/var/log/mongodb/mongo.log
     logAppend: true
 storage:
     dbPath: /usr/local/var/mongodb
 net:
     bindIp: 127.0.0.1
 replication:
     replSetName: rs0
     oplogSizeMB: 100
</code></pre></li>
</ol>


<!--more-->


<ol>
<li><p>Restart mongodb service</p>

<pre><code> brew services restart mongodb
</code></pre>

<p> <br/></p></li>
<li><p>log on to <code>mongo</code> shell and initiate replica set</p>

<pre><code> rs.initiate({_id: "rs0", members: [{_id: 0, host: "127.0.0.1:27017"}] })
</code></pre>

<p>verify replication members with <code>rs.status()</code></p></li>
</ol>


<h3>Mongodb Ops Manager</h3>

<p>Ops Manager is a management application from mongodb and its available as part of MongoDB Enterprise. Ops Manager enables to configure and maintain MongoDB nodes and clusters.</p>

<p><strong><em>Server Pool</em></strong></p>

<p>Ops Manager Server Pool allows Ops Manager users with administrative privileges, i.e. Ops Manager Administrators, to maintain a pool of provisioned servers that already have installed. When users in a project want to create a new MongoDB deployment, they can request servers from this pool to host the MongoDB deployment.</p>

<p>If you manage large or multiple MongoDB clusters, you probably find yourself scaling them up, and perhaps down, on demand. And if you’re providing MongoDB as a service to your developer teams, scaling requirements might come with little to no notice.</p>

<p>With a few clicks in the Ops Manager GUI, or via a simple API call, you can add new nodes to a cluster, and remove then when they are no longer needed. MongoDB automatically rebalances data as your topology changes, all without service impact.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rails Applicatoin - Openshift 3.0 deployment]]></title>
    <link href="http://vishnuatrai.in/blog/2016/12/05/rails-applicatoin-openshift-3-dot-0-deployment/"/>
    <updated>2016-12-05T18:34:50+05:30</updated>
    <id>http://vishnuatrai.in/blog/2016/12/05/rails-applicatoin-openshift-3-dot-0-deployment</id>
    <content type="html"><![CDATA[<h3>Prepare helper scripts for application setup, app start and httpd config</h3>

<p>Helper scripts can be stored as part of application code in directory <code>scripts</code> in project root directory.</p>

<p>Sample application setup script</p>

<pre><code># scripts/application_setup.sh

#! /bin/bash
bundle exec rake db:migrate
bundle exec rake db:seed
</code></pre>

<!--more-->


<p></p>

<p>Sample applicaiton startup script</p>

<pre><code># scripts/appStartup.sh

#!/bin/bash
bundle exec rake assets:precompile

config-httpd
exec httpd -D FOREGROUND
</code></pre>

<p>Sample script to configure httpd.conf</p>

<pre><code># scripts/config_httpd.sh

#! /bin/sh

sed -i "s/RailsEnv RAILS_ENV/RailsEnv ${RAILS_ENV}/" /etc/httpd/conf/httpd.conf
echo "PassengerMinInstances 12" &gt;&gt; /etc/httpd/conf/httpd.conf
echo "PassengerMaxPoolSize 12" &gt;&gt; /etc/httpd/conf/httpd.conf
ruby -S passenger-config build-native-support
</code></pre>

<p><code>httpd.conf</code> for passenger apache config will include VirtualHost and passenger configs ie. PassengerRoot, PassengerRuby etc.</p>

<h3>Docker Image build with Rails application setup</h3>

<p>Sample Dockerfile to build image with rails app. Dockerfile will reside in project root directory.</p>

<pre><code>FROM centos/passenger-40-centos7
RUN mkdir -p /opt/app-root/bundle
COPY ./Gemfile /opt/app-root/bundle/Gemfile
COPY ./Gemfile.lock /opt/app-root/bundle/Gemfile.lock

# install sqlite3 on same app image as db, this should be run as seperate docker container service
RUN sudo apt-get install -y sqlite3 libsqlite3-dev

WORKDIR /opt/app-root/bundle
RUN scl enable rh-ruby25 'bundle install --deployment --without capistrano:development:test:int_test --jobs=4'
RUN chmod -R g+w /opt/app-root/bundle/

# Copy application code
COPY . /opt/app-root/src

WORKDIR /opt/app-root/src
# Ensure log directory writable, move in the bundle, 
# set app's default group as root, and put the app startup script in place
RUN chmod -R g+w /opt/app-root/src/log /opt/app-root/src/db /opt/app-root/src/config /opt/app-root/src/public
RUN mkdir -p /opt/app-root/src/tmp
RUN chmod 777 /opt/app-root/src/tmp /opt/app-root/src/log /opt/app-root/src/public
RUN ln -s /opt/app-root/bundle/.bundle /opt/app-root/src/.bundle
RUN mkdir -p /opt/app-root/src/vendor
RUN ln -s /opt/app-root/bundle/vendor/bundle /opt/app-root/src/vendor/bundle

COPY scripts/application_setup.sh /usr/bin/application_setup.sh
COPY scripts/appStartup.sh /usr/bin/appStartup.sh
COPY scripts/config_httpd.sh /usr/bin/config_httpd.sh

COPY scripts/httpd.conf /etc/httpd/conf/httpd.conf

CMD ["/usr/bin/appStartup.sh"]
</code></pre>

<h3>Build Docker image using Dockerfile and push the image to docker image registry</h3>

<h3>Openshift Deployment config</h3>

<p>Using this deployment template we can build a deployment on openshift, these template files can be part of source code in template direcotry of project root dir.</p>

<pre><code>     oc process -f templates/deployment_config.yml | oc create -f -
</code></pre>

<p>Sample  <code>DeploymentConfig</code> template</p>

<pre><code>  # templates/deployment_config.yml

  apiVersion: v1
  kind: DeploymentConfig
  metadata:
    name: sample-rails-app
    labels:
      app: sample-rails-app
  spec:
    strategy:
      type: Recreate
      recreateParams:
        timeoutSeconds: 600
        mid:
          failurePolicy: Abort
          execNewPod:
            command:
            - application_setup.sh
            containerName: sample-rails-app
    triggers:
    - type: ConfigChange
    - type: ImageChange
    replicas: 1
    selector:
      app: sample-rails-app
      deploymentconfig: sample-rails-app
    template:
      metadata:
        name: sample-rails-app
        labels:
          app: sample-rails-app
          deploymentconfig: sample-rails-app
      spec:
        containers:
        - name: sample-rails-app
          image: 'docker.hub.com/railsapp/sample-rails-app:0.1'
          ports:
          - containerPort: 8080
          env:
          - name: RAILS_ENV
            value: dev
        restartPolicy: Always
</code></pre>

<p>Using service template a service can be created that will be mapped to deployment pods.</p>

<pre><code>    oc process -f templates/service_config.yml | oc create -f -
</code></pre>

<p>sample <code>Service</code> template</p>

<pre><code>  # templates/service_config.yml    
  apiVersion: v1
  kind: Service
  metadata:
    name: sample-rails-app
    labels:
      app: sample-rails-app
  spec:
    ports:
    - name: web
      port: 8080
      targetPort: 8080
    selector:
      app: sample-rails-app
      deploymentconfig: sample-rails-app
</code></pre>

<p>Using routes services will be exposed to external clients, service will be mapped to a route and route will be available to external world.</p>

<pre><code>oc process -f templates/route_config.yml | oc create -f -
</code></pre>

<p>sample <code>Route</code> template</p>

<pre><code>  # templates/route_config.yml
  apiVersion: v1
  kind: Route
  apiVersion: v1
  metadata:
    name: sample-rails-app
    labels:
      app: sample-rails-app
  spec:
    host: sample-rails-app.openshift-cluster.com
    to:
      kind: Service
      name: sample-rails-app
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is new in Rails 5]]></title>
    <link href="http://vishnuatrai.in/blog/2016/07/05/what-is-new-in-rails-5/"/>
    <updated>2016-07-05T17:34:19+05:30</updated>
    <id>http://vishnuatrai.in/blog/2016/07/05/what-is-new-in-rails-5</id>
    <content type="html"><![CDATA[<h3>ApplicationRecord</h3>

<p>Similar to <code>ApplicationController</code> which is the common base class for all controllers that you get with new Rails apps, <code>ApplicationRecord</code> will provide a base class for your ActiveRecord models. This provides a common place to put any base model concerns.
<code>app/models/application_record.rb</code> file will be automatically added to models in Rails 5 applications.</p>

<pre><code># app/models/application_record.rb
class ApplicationRecord &lt; ActiveRecord::Base
    self.abstract_class = true
end
</code></pre>

<h3>ActionCable</h3>

<p>Action Cable can integrates websocket with rails application. Action Cable server can handle multiple connection instances. It has only one instance per websocket connection. The client websocker connection(consumer) can subscribe to multiple cable channels.<!--more-->For example Action Cable server can have a <code>ChatChannel</code> and an <code>AppearancesChannel</code> and a consumer(websocket) can subscribe to either one or both of channels.</p>

<pre><code>#Publisher Streams
# app/channels/chat_channel.rb
class ChatChannel &lt; ApplicationCable::Channel
    def subscribed
        stream_from "chat_#{params[:room]}"
    end
end

#Subscriber
// app/javascript/channels/chat_channel.js
import consumer from "./consumer"
consumer.subscriptions.create({ channel: "ChatChannel", room: "Best Room" })
</code></pre>

<h3>ActiveRecord::Attributes</h3>

<p>Define an <code>attribute</code> on a model with type. It is not essential to have a database column with the custom model <code>attribute</code>. <code>attribute</code> can also be used to provide default values.</p>

<pre><code># db/schema.rb
create_table :profiles, force: true do |t|
    t.decimal :gpa
    t.string :name, default: "Full Name"
end

# app/models/profile.rb
class Profile &lt; ActiveRecord::Base
end

profile = Profile.new(gpa: '4.1')

# before
profile.gpa # =&gt; BigDecimal.new(4.1)
profile.new.name # =&gt; "Full Name"

class Profile &lt; ActiveRecord::Base
    attribute :gpa, :integer # custom type
    attribute :name, :string, default: "Your Name" # default value
    attribute :current_time, :datetime, default: -&gt; { Time.now } # default value
    attribute :field_without_db_column, :integer, array: true
end

# after
profile.gpa # =&gt; 10
Profile.new.name # =&gt; "Your Name"
Profile.new.current_time # =&gt; 2015-05-30 11:04:48 -0600
model = Profile.new(field_without_db_column: ["1", "2", "3"])
model.attributes # =&gt; {field_without_db_column: [1, 2, 3]}
</code></pre>

<h3>Rails API &ndash; ActionController::API</h3>

<p>To avoid middlewares used for browser based web applications and server public facing json APIs, we can create API only rails application here after 5.0. <code>rails new my_api --api</code> can be used to generate application which will be API only and will not generate assets and views.</p>

<pre><code># app/controllers/application_controller.rb
class ApplicationController &lt; ActionController::API
end
</code></pre>

<h3>Ruby 2.2.2+ required version from Rails 5.0</h3>

<p>Ruby 2.2.2+ required version from Rails 5.0</p>

<br />


<br />


<h3>References</h3>

<p><a href="https://guides.rubyonrails.org/5_0_release_notes.html">https://guides.rubyonrails.org/5_0_release_notes.html</a></p>
]]></content>
  </entry>
  
</feed>
