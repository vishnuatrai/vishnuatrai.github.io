<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[@vishnuatrai]]></title>
  <link href="http://vishnuatrai.in/atom.xml" rel="self"/>
  <link href="http://vishnuatrai.in/"/>
  <updated>2020-04-04T16:37:47+05:30</updated>
  <id>http://vishnuatrai.in/</id>
  <author>
    <name><![CDATA[vishnuatrai.in]]></name>
    <email><![CDATA[vishnu.atrai@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[MySQL - Encryption at rest]]></title>
    <link href="http://vishnuatrai.in/blog/2020/01/11/mysql-encryption-at-rest/"/>
    <updated>2020-01-11T18:20:46+05:30</updated>
    <id>http://vishnuatrai.in/blog/2020/01/11/mysql-encryption-at-rest</id>
    <content type="html"><![CDATA[<p>Without encryption of data at rest, system role with access to file system can view data even without proper database permissions. A proper authentication can be used to protect data used by an application but data sitting in file system has been area of risk.</p>

<p>With encryption at rest, it has been possible to protect data in transit when moving over the network. Data at Rest Encryption is a requirement for HIPAA, PCI regulations. <!--more--></p>

<h3>Data Files at Risk</h3>

<p>When insert queries run to store data in tables, the data will be stored in respective <code>dataDir</code> ie <code>/usr/local/var/mysql</code>. A system user that has filesystem access can use <code>strings</code> or <code>xxd</code> commands to view content in data files, bin logs or redo logs.</p>

<p>One way to avoid this risk either not to store sensitive data in database or encrypt sensitive data columns or encrypt the file system itself.</p>

<p>Other way is to implement Data encryption at rest.</p>

<h3>Implement Data Encryption at rest, MySQL</h3>

<p>First step to create a key using <code>openssl</code> command</p>

<pre><code>openssl enc -aes-256-cbc -md sha1 -k &lt;enc password or passphase&gt; -in keys.txt -out mysql.enc
</code></pre>

<p>Second, edit <code>/etc/my.cnf</code> and provide encryption configurations</p>

<pre><code>plugin-load-add=file_key_management.so
file-key-management-filename=/var/lib/mysql/keys.txt
innodb-encrypt-tables
innodb-encrypt-log
encrypt-binlog
</code></pre>

<p><code>plugin-load-add</code> option will load the encryption plugin on mysql start.<br/>
<code>file-key-management-filename</code> option to provide reference to encryption key file.<br/>
<code>innodb-encrypt-tables</code> option will encrypt tables by default.<br/>
<code>innodb-encrypt-log</code> option will encrypt redo logs by default.<br/>
<code>encrypt-binlog</code> option will encrypt binlogs.</p>

<p>Third, restart mysql</p>

<pre><code>service mysql restart
</code></pre>

<p>Fourth, enable the same options on the replication slaves</p>

<p>Fifth, verify tables are encrypted using below mysql query</p>

<pre><code>select * from information_schema.INNODB_TABLESPACES_ENCRYPTION where ENCRYPTION_SCHEME=1;
</code></pre>

<br/>


<h3>innodb-encrypt-tables modes</h3>

<p>Three ways to encrypt tables<br/></p>

<ol>
<li><code>innodb-encrypt-tables</code> option in <code>my.cnf</code> will encrypt all tables unless table creation statement provided &ldquo;encrypted=no&rdquo; <br/></li>
<li><code>innodb-encrypt-tables</code> option in <code>my.cnf</code> will encrypt all tables and will not allow table creation with &ldquo;encrypted=no&rdquo; <br/></li>
<li><code>innodb-encrypt-tables</code> option is not provided in <code>my.cnf</code> but the plugin presents, it can explicitely encrypt a table by including &ldquo;encrypted=yes&rdquo; in table creation statement.<br/></li>
</ol>


<h3>Performance Overhead</h3>

<p>~5% to ~10% performance overhead observed in previous MySQL deployments with database enforced encryption mode.</p>

<p>Database-level encryption and selective tables encryption can be a better option than the filesystem-level encryption.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dive into Docker Image Layers]]></title>
    <link href="http://vishnuatrai.in/blog/2019/12/26/dive-into-docker-image-layers/"/>
    <updated>2019-12-26T20:48:58+05:30</updated>
    <id>http://vishnuatrai.in/blog/2019/12/26/dive-into-docker-image-layers</id>
    <content type="html"><![CDATA[<p>I recently used a tool named <a href="https://github.com/wagoodman/dive"><code>dive</code></a> to explore docker images developed and maintained by teammates or opensourced docker images. I used to look into <code>Dockerfile</code> steps to explore image layes. <code>dive</code> brings cli-ui and using arrow and tab keys we can explore each layer in left pane while layer contents listed in right pane.<!--more--></p>

<p>Essentially <code>dive</code> based on docker api and its cache contents and provides commands to explore layer contents and minimize the size of images.</p>

<p>Installation</p>

<p><code>brew install dive</code></p>

<p>command to explore a image layers</p>

<p><code>dive &lt;docker-image-id&gt;</code></p>

<p>to build a image using <code>dive</code> command</p>

<p><code>dive build -t &lt;image-tag&gt; .</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[jMeter - Optimize for load testing]]></title>
    <link href="http://vishnuatrai.in/blog/2019/11/11/jmeter-optimize-for-load-testing/"/>
    <updated>2019-11-11T17:40:57+05:30</updated>
    <id>http://vishnuatrai.in/blog/2019/11/11/jmeter-optimize-for-load-testing</id>
    <content type="html"><![CDATA[<p>These are the jMeter optimization steps we have taken to simulate 1M request per second concurrency test for REST application deployed on Kubernetes cluster.</p>

<h3>Use a High Capacity Jenkins Slave to run jMeter jmx script</h3>

<p>We have added a high capacity Jenkins slave node to run only jMeter performance test and have done jMeter installation on that server. All performance test jobs are restricted to run on that labeled slave node.<!--more-->
We can use available memory and CPU to optimize jMeter configuration.</p>

<h3>Run jMeter in non-GUI Mode</h3>

<p>Apart from developing jmx script or debugging request/response not use GUI mode to run load test. In GUI mode AWT Event Thread will disrupt both your test and JMeter in case of more or less high load. GUI mode consumes a lot of memory and other resources, which in turn negatively impacts your scripts and tests. Using the non-GUI mode of JMeter helps to reduce both resource requirements and potential errors.</p>

<h3>Increase the Java Heap Size</h3>

<p>Run JMeter with higher value of memory.</p>

<pre><code>`JVM_ARGS="-Xms512m -Xmx512m" jmeter.sh`
</code></pre>

<h3>Avoid Listeners</h3>

<p>Avoid UI listeners like graphs or results table to avoid OutOfMemory issues. Preferably only write results to a JTL.</p>

<h3>Minimize Metrics Need to Store</h3>

<p>Configure JMeter to ensure that it will only save the metrics that you absolutely need. You can control what to store by adding relevant lines to the user.properties file in jMeter installation.</p>

<h3>Generate Reports AFTER the Run</h3>

<p>It takes resources to be written (CPU and memory) and for analysis in XML format. Use the outputted .jtl files to create reports once the load test is finished. Building the report requires a great amount of CPU and memory resources.</p>

<h3>Tweak the JVM</h3>

<p>Settings like garbage collector (-XX:+UseConcMarkSweepGC), server JVM (-server) can be set inside JVM_ARGS by editing the JMeter launcher script.</p>

<p><code>NEW="-XX:NewSize=128m -XX:MaxNewSize=128m"</code> line in the JMeter command script should match with values provided in HEAP.</p>

<p><code>-XX:+UseConcMarkSweepGC</code> &ndash; this forces the usage of the CMS garbage collector. It will lower the overall throughput but leads to much shorter CPU intensive garbage collections.</p>

<p><code>-server</code> &ndash; this switches JVM into “server” mode with runtime parameters optimization. In this mode JMeter starts more slowly, but your overall throughput will be higher</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is new in Rails 6]]></title>
    <link href="http://vishnuatrai.in/blog/2019/09/15/what-is-new-in-rails-6/"/>
    <updated>2019-09-15T14:09:12+05:30</updated>
    <id>http://vishnuatrai.in/blog/2019/09/15/what-is-new-in-rails-6</id>
    <content type="html"><![CDATA[<h4>Rails 6 require Ruby version 2.5 or greater. Upgrade to at least rails 5.2 or later versions and make sure application run properly without any failure, then attempt to upgrade to 6.0, follow <a href="https://edgeguides.rubyonrails.org/upgrading_ruby_on_rails.html#upgrading-from-rails-5-2-to-rails-6-0">upgrade guide</a>.</h4>

<h3>ApplicationRecord &ndash; Multiple databases support</h3>

<p>Active Record supports switching between multiple databases with a minimal change in codebase but a major impact on multiple replica set db. With multiple db support, rails application can have a read-only version of your database to use in areas known for having slow queries, or if it need to write to different databases depending on which controller request.
This requires a change in <code>database.yml</code> setup, ie. <!--more-->
`</p>

<pre><code>development:
  primary:
    database: primary_db
    user: rw_user
  primary_replica:
    database: primary_db
    user: ro_user
    replica: true
  animals:
    database: animals_db
    user: rw_user
  animals_replica
    database: animals_db
    user: ro_user
    replica: true
</code></pre>

<p>`
You can then specify at the model-level which database(s) you want to use:</p>

<pre><code>class Animal &lt; ApplicationRecord
    connects_to database: { writing: :animals, reading: :animals_replica }
end
</code></pre>

<p>And then it’s just one line of code to temporarily switch between databases inside a block!</p>

<pre><code>Aminal.connected_to(role: :reading) do
    #Slow queries in jobs can be performaed here
end
</code></pre>

<h3>Action Mailbox</h3>

<p>ActionMailbox provides a set of tools that will allow applications to route incoming mail into controller-like mailboxes for processing. Action Mailbox requires Active Job and Active Storage as part of it’s and a database table <code>InboundEmail</code>.</p>

<h4>Setup</h4>

<pre><code>bin/rails action_mailbox:install
bin/rails db:migrate
</code></pre>

<h4>Routing and Processing</h4>

<pre><code># app/mailboxes/application_mailbox.rb
class ApplicationMailbox &lt; ActionMailbox::Base
    routing /^comment\+(.+)@example\.com/i =&gt; :comments
end
# app/mailboxes/comments_mailbox.rb
class CommentsMailbox &lt; ApplicationMailbox
    def process
        user = User.find_by(email: mail.from)
        post_uuid = COMMENTS_REGEX.match(mail.to)[1]

        post = Post.find_by(uuid: post_uuid)
        post.comments.create(user: user, content: mail.body)
    end
end
</code></pre>

<h3>Action Text</h3>

<p>ActionText is am implementation of rich-text support(Trix editor by Basecamp). Run bin/rails action_text:install to add the Yarn package and copy over the necessary migration. Also, you need to set up Active Storage for embedded images and other attachments. Both trix and @rails/actiontext should be required in your JavaScript pack.</p>

<pre><code>// application.js
require("trix")
require("@rails/actiontext")
</code></pre>

<p>It exposes a has_rich_text method that we can apply to models and Action Text will take care of the rest.</p>

<pre><code># app/models/article.rb
class Article &lt; ApplicationRecord
    has_rich_text :content
end
</code></pre>

<h3>Parallel Testing</h3>

<p>Rails 6 adds parallel test to Rails applications by default. It parallelize test suite and enable faster test suite run times and efficiency. Configuration via parent test class</p>

<pre><code>class ActiveSupport::TestCase
    parallelize(workers: 2, with: :processes) # or :threads
end
</code></pre>

<p>or through environment variable PARALLEL_WORKERS and it’ll create the database with a numbered suffix.</p>

<pre><code>PARALLEL_WORKERS=2 rails test
</code></pre>

<h3>Action Cable Testing</h3>

<p>Action Cable testing tools allow you to test your Action Cable functionality at any level: connections, channels, broadcasts. By default, when you generate new Rails application with Action Cable, a test for the base connection class (<code>ApplicationCable::Connection</code>) is generated as well under <code>test/channels/application_cable</code> directory.</p>

<h3>Webpack</h3>

<p><code>webpacker</code> gem, replacing the previously-default Rails Asset Pipeline and providing for better interoperability with modern JavaScript libraries and coding standards. Using webpacker gem, all StyleSheets, images and JS libraries wrap into a single bundle with a single access point.</p>

<h3>Zeitwerk</h3>

<p>Zeitwerk is a new and improved, thread-safe code loader for Rails, Configuration to enable &ndash;</p>

<pre><code>config.autoloader = :zeitwerk
</code></pre>

<p><a href="https://edgeguides.rubyonrails.org/6_0_release_notes.html">https://edgeguides.rubyonrails.org/6_0_release_notes.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Jenkins Slave with Docker Executors]]></title>
    <link href="http://vishnuatrai.in/blog/2019/08/08/jenkins-slave-with-docker-executors/"/>
    <updated>2019-08-08T16:35:25+05:30</updated>
    <id>http://vishnuatrai.in/blog/2019/08/08/jenkins-slave-with-docker-executors</id>
    <content type="html"><![CDATA[<p><a href="https://wiki.jenkins.io/display/JENKINS/Docker+Plugin">Docker Plugin</a> enables jenkins to run jobs as docker container. In such case we dont need to setup jenkins nodes(agents) with specific binaries, instead docker images will be used to run jobs.<!--more-->Jenkins master needs to be configured with docker host where we can push docker images, those will be used by agents to execute jobs.</p>

<h3>Global Configuration</h3>

<p>After installation of <code>Docker Plugin</code> from manage plugins options, we will need to configure docker host and templates to create docker executers in Global Configuration option.</p>

<p>Go to <strong>Global Configurations</strong> &ndash;> <strong>Docker</strong> &ndash;> fill the below configuration options to setup docker host <br />
<strong>Name</strong>    Name of docker slave to be used in job configuration  <br />
<strong>Docker Url</strong>    docker engine url and port  <br /></p>

<p><img src="http://vishnuatrai.in/images//posts/Jenkins7.png" alt="" /></p>

<p><strong>Docker Template</strong>    docker template is an executor, we can add multiple templates and it will enable multiple executors  <br />
Fill below configurations to setup docker template <br /></p>

<ul>
<li><strong>Docker Image</strong> image name that that is available on docker host and having binaries to run the job</li>
<li><strong>Container Settings</strong> > <strong>Volumes</strong> Provide the container volumes(ie. <code>/home/dockerslave/bin</code>) where the binaries available, these are mapped with host machine and binaries will be available on host machine to run the job.</li>
<li><strong>Label</strong> Provide the label to uniquely identify the executor and configure in job configuration</li>
<li><strong>Launch Method</strong> select <strong>Docker ssh compute launcher</strong></li>
<li><strong>Credentials</strong> select credential id to ssh into docker host container</li>
</ul>


<p><img src="http://vishnuatrai.in/images//posts/Jenkins4.png" alt="" />
<img src="http://vishnuatrai.in/images//posts/Jenkins5.png" alt="" />
<img src="http://vishnuatrai.in/images//posts/Jenkins6.png" alt="" /></p>

<h3>Job Configuration</h3>

<p><strong>Build</strong> > check <strong>Restrict where this project can be run</strong> > <strong>Label Expression</strong> > provide the docker template lable</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Jenkins - Add Slave nodes as JNLP agents]]></title>
    <link href="http://vishnuatrai.in/blog/2019/04/15/jenkins-add-slave-nodes-as-jnlp-agents/"/>
    <updated>2019-04-15T15:36:54+05:30</updated>
    <id>http://vishnuatrai.in/blog/2019/04/15/jenkins-add-slave-nodes-as-jnlp-agents</id>
    <content type="html"><![CDATA[<p>If it is required to run Jenkins master in an isolated network and master should not be allowed to connected to its nodes(agents), we can use JNLP method to add agents to master to process jobs in distributed manner. In this scenario, its not desirable to have master connections with slave(agent) nodes but agent to master connections only required. <!--more--></p>

<h3>Only Agent to master connections</h3>

<p>In this case the agent node will not be visible to the master, so the master can not initiate the agent process. You can use a different type of agent configuration in this case called &ldquo;JNLP&rdquo;. This means that the master does not need network &ldquo;ingress&rdquo; to the agent (but the agent will need to be able to connect back to the master). Handy for if the agents are behind a firewall, or perhaps in some more secure environment to do trusted deploys.</p>

<h3>Configuration</h3>

<p>In order to setup a slave agent in above scenario you need to first <strong>Enable the JNLP Agents</strong>:</p>

<p>Go to <strong>Manage Jenkins</strong> &ndash;> <strong>Configure Global Security</strong> &ndash;> under <strong>Agents</strong> section &ndash;> <strong>TCP port for inbound agents</strong> &ndash;> select <strong>Random</strong> &ndash;><strong>Save</strong>.</p>

<p><img src="http://vishnuatrai.in/images//posts/Jenkins1.png" alt="" /></p>

<h4>Setup slave agent node</h4>

<p>Go to <strong>Manage Jenkins</strong> &ndash;> <strong>Manage Nodes</strong> &ndash;>click on <strong>New Node</strong> &ndash;> Enter the <strong>node name</strong> &ndash;> Select <strong>permanent agent</strong>.</p>

<p>Fill the below details to configure the slave agent</p>

<h4>Description</h4>

<h4>Remote root directory</h4>

<p>This should be the workspace directory on slave agent</p>

<h4>Label</h4>

<p>Provide the label to uniquely identify the slave node</p>

<h4>Launch Method</h4>

<p>Select <strong>launch agent by Connecting it to the master</strong> for windows agents  <br />
and <strong>launch agents via ssh</strong> for linux agents</p>

<p><img src="http://vishnuatrai.in/images//posts/Jenkins2.png" alt="" /></p>

<p>To Launch the slave agent via command line  <br />
<strong>Download the agent.jar file and copy to agent node</strong>  <br />
<strong>Run agent.jar using command line</strong></p>

<pre><code>java --jar agent.jar -jnlpUrl &lt;jenkins master url&gt; -secret &lt;secret given on node configuration&gt; -workDir "/user/agent/home/workspace"
</code></pre>

<p><img src="http://vishnuatrai.in/images//posts/Jenkins3.png" alt="" /></p>

<p>With above command agent is authorized and registered with jenkins master. Now agent is successfully configured and launched, which can be verified on master nodes. Jenkins master can delegate jobs to agent node.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MongoDB 4.0 - Support for ACID transactions]]></title>
    <link href="http://vishnuatrai.in/blog/2018/12/20/mongodb-4-dot-0-support-for-acid-transactions/"/>
    <updated>2018-12-20T17:41:34+05:30</updated>
    <id>http://vishnuatrai.in/blog/2018/12/20/mongodb-4-dot-0-support-for-acid-transactions</id>
    <content type="html"><![CDATA[<h3>Support for Single-Shard multi document ACID transactions</h3>

<p>To support benefits of ACID transactions with fully-denormalized document data modeling, MongoDB added single-shard ACID transaction support in the released 4.0. These single-shard transactions apply ACID properties on updates across multiple documents those are present in the same shard. Multi-shard transactions supposed to implement in release 4.2.</p>

<h4>Scenario, how it works?</h4>

<ol>
<li>Client application will get a database session <!--more--></li>
<li><p>Database session initiate transaction block using <code>start_transaction</code> statement with <code>majority</code> <code>writeConcern</code> in <code>try</code> block. The only <code>writeConcern</code> thats suitable for high data durability is that of majority. This means a majority of replicas should commit the changes before the primary acknowledges the success of the write to the client. The transaction will remain blocked till at least 1 of the 2 secondaries pulls the update from the primary using asynchronous replication which is susceptible to unpredictable replication lag especially under heavy load.</p></li>
<li><p>Within transaction block multiple insert/update/remove statements can be made.</p></li>
<li>At end of transaction block <code>commit_transaction</code> required to commit data in database.</li>
<li>If any exception occurs in transaction block, client application should <code>abort_transaction</code> in <code>catch</code> block to roll back updates.</li>
<li>Finally the database client session can be closed in <code>finally</code> block</li>
</ol>


<p>Sample Java code to mimic same scenario,</p>

<pre><code>###Java sample
private void multiDocumentTransactions() {
    ClientSession session = client.startSession();
    try {
        session.startTransaction(TransactionOptions.builder().writeConcern(WriteConcern.MAJORITY).build());
        orders.updateOne(session, {..filter..}, {..updates..});
        stock.updateOne(session, {..filter..}, {..updates..});
        session.commitTransaction();
    } catch (MongoCommandException e) {
        session.abortTransaction();
        //ROLLBACK TRANSACTION
    } finally {
        session.close();
    }
}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ProxySQL - MySQL High Availability Load Balancing]]></title>
    <link href="http://vishnuatrai.in/blog/2018/11/12/proxysql-mysql-high-availability-load-balancing/"/>
    <updated>2018-11-12T22:59:40+05:30</updated>
    <id>http://vishnuatrai.in/blog/2018/11/12/proxysql-mysql-high-availability-load-balancing</id>
    <content type="html"><![CDATA[<h3>What is ProxySQL?</h3>

<ul>
<li>Lightweight Proxy layer for MySQL</li>
<li>Its protocol aware that we put between application and database server</li>
<li>Improve database operations <!--more--></li>
<li>Understand and solve performance issues</li>
<li>A proxy layer to shield the database</li>
<li>Add high availability to database topology</li>
</ul>


<h3>Why ProxySQL/HAProxy?</h3>

<p>ProxySQL provides below advantages if included in deployment stack</p>

<ul>
<li>Scalability

<ul>
<li>Connection Pooling and multiplexing</li>
<li>Read/Write split</li>
<li>Read/Write sharding</li>
</ul>
</li>
<li>High Availability

<ul>
<li>Seamless failover</li>
<li>Load balancing</li>
<li>Cluster aware</li>
</ul>
</li>
<li>Advance query and support

<ul>
<li>Query caching</li>
<li>Query rewrite</li>
<li>Query blocking</li>
<li>Query mirroring</li>
<li>Query throttling</li>
<li>Query timeout</li>
</ul>
</li>
<li>Manageability

<ul>
<li>Admin Utility</li>
<li>Runtime reconfiguration</li>
<li>Monitoring</li>
</ul>
</li>
</ul>


<h3>ProxySQL Configurations Options</h3>

<pre><code>mysql_replication_hostgroups
mysql_servers
mysql_server_connect_log
mysql_server_ping_log
mysql_server_replication_lag_log
mysql_server_read_only_log
stats_mysql_connection_pool
stats_mysql_commands_counters
mysql_query_rules
stats_mysql_query_digest
</code></pre>

<h3>Deploying with Kubernetes/Openshift &ndash; The Sidecar Pattern</h3>

<p>We can deploy the <code>proxysql</code> image in same pod as application image but data base references we will need to provide as <code>localhost</code> or <code>127.0.0.1</code> and port will be <code>6033</code> (the default proxysql port). The application will connect to proxysql server and it will then redirect database queries to mysql database cluster.</p>

<p>Openshift/kubernetes template spec &ndash;</p>

<pre><code>spec:
 template:
  spce:
   volumes:
    -
   name: "proxysql-configiguration-file"
     secret:
      secretName: "proxysql-configuration"
   containers:
    -
   name: "proxysql-service"
     image: "docker.com/proxysql/proxysql:1.0"
     ports:
      -
   containerPort: 6033
       protocal: TCP
   env:
   resources:
   volumeMounts:
    -
   name: "proxysql-configuration"
     readOnly: true
     mountPath: "/etc/prosysql-config"
</code></pre>

<h3>References</h3>

<p><a href="https://www.proxysql.com/">https://www.proxysql.com/</a></p>

<p>docker sandbox</p>

<p><a href="https://github.com/vishnuatrai/MySQLSandbox">https://github.com/vishnuatrai/MySQLSandbox</a></p>

<p>github</p>

<p><a href="https://github.com/sysown/proxysql">https://github.com/sysown/proxysql</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MongoDB Change Streams - Real time oplog notifications]]></title>
    <link href="http://vishnuatrai.in/blog/2018/07/05/mongodb-change-streams-real-time-notifications-to-subscribers-for-data-updates/"/>
    <updated>2018-07-05T17:19:31+05:30</updated>
    <id>http://vishnuatrai.in/blog/2018/07/05/mongodb-change-streams-real-time-notifications-to-subscribers-for-data-updates</id>
    <content type="html"><![CDATA[<p>Change Stream is easiest way to subscribe database changes realtime, its based on mongoDB oplog(Operation Log) technology. An ideal use case would be ETL operational data to a reporting and visualization data store via kafka data pipeline.</p>

<h3>MongoDB real time data sync technologies</h3>

<ol>
<li>mongodb oplog technology based changestream enable applications to stream real-time data changes<!--more--></li>
<li>changestream can notify your application of all writes to documents (including deletes) without polling.</li>
<li>changestream advantages over oplog</li>
</ol>


<h2>Use case</h2>

<p>MongoDB changestream (operation logs, create, update, delete) will be as data source. Java springboot based application to subscribe changes and push to kafka topic. Kafka data pipeline can refine data, transform, filter and sync in to a big data storage for reporting, visualization or many other purpose.</p>

<pre><code>###Java sample
import com.mongodb.Block;
import com.mongodb.MongoClient;
import com.mongodb.MongoClientURI;
import com.mongodb.client.MongoCollection;
import com.mongodb.client.MongoDatabase;
import com.mongodb.client.model.changestream.FullDocument;
import com.mongodb.client.model.changestream.ChangeStreamDocument;
import org.bson.Document;


MongoClient mongoClient = new MongoClient(new MongoClientURI("mongodb://localhost:27017,localhost:27018,localhost:27019"));
MongoDatabase database = mongoClient.getDatabase("test");
MongoCollection&lt;Document&gt; collection = database.getCollection("restaurants");

Block&lt;ChangeStreamDocument&lt;Document&gt;&gt; printBlock = new Block&lt;&gt;() {
@Override
    public void apply(final ChangeStreamDocument&lt;Document&gt; changeStreamDocument) {
        System.out.println(changeStreamDocument);
    }
};

collection.watch().forEach(printBlock);
</code></pre>

<h3>changestream advantages over oplog</h3>

<ol>
<li><strong><em>Use access control</em></strong> (no admin user required to access changestream)</li>
<li><strong><em>Present a Defined API</em></strong>, API syntax takes advantage of the established MongoDB query language, and are independent of the underlying oplog format.</li>
<li><strong><em>Total Ordering</em></strong>, subscriber applications will always receive changes in the order they were applied to the database.</li>
<li><strong><em>Filters</em></strong>, Changes can be filtered to provide relevant and targeted changes to listening applications. As an example, filters can be on operation type or fields within the document.</li>
<li><strong><em>Power of aggregation</em></strong>, define change streams on any collection just like any other normal aggregation operators using <code>$changeStream</code> operator and <code>watch()</code> method</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is new in Rails 5.2]]></title>
    <link href="http://vishnuatrai.in/blog/2018/01/20/what-is-new-in-rails-5-dot-2/"/>
    <updated>2018-01-20T16:02:51+05:30</updated>
    <id>http://vishnuatrai.in/blog/2018/01/20/what-is-new-in-rails-5-dot-2</id>
    <content type="html"><![CDATA[<h3>Active Storage</h3>

<p>Active Storage supports modern approach for file uploading to Amazon S3, Google Cloud Storage, Microsoft Azure Cloud file storage. It will also provide references to active record database tables <code>active_storage_blobs</code> and <code>active_storage_attachments</code>. <code>rails active_storage:install</code> will install initial setup for active storage.<!--more-->Configure and setup <code>config/storage.yml</code> cloud credentials and storage buckets.</p>

<pre><code>#config/storage.yml
local:
  service: Disk
  root: &lt;%= Rails.root.join("storage") %&gt;
test:
  service: Disk
  root: &lt;%= Rails.root.join("tmp/storage") %&gt;
amazon:
  service: S3
  access_key_id: ""
  secret_access_key: ""
  bucket: ""
  region: ""
</code></pre>

<h3>Redis Cache Store</h3>

<p>Rails 5.2 ships with built-in Redis cache store. The Redis cache store takes advantage of Redis support for automatic eviction when it reaches max memory, allowing it to behave much like a Memcached cache server.</p>

<p>Finally, add the configuration in the relevant <code>config/environments/*.rb</code> file:</p>

<pre><code>config.cache_store = :redis_cache_store, { url: 'redis://redis-server:6379' }
</code></pre>

<h3>HTTP/2 Early Hints</h3>

<p>This means we can automatically instruct the web server to send required style sheet and JavaScript assets early. Which means faster full page delivery.</p>

<p>To start the server with Early Hints enabled pass <code>--early-hints</code> to <code>rails server</code></p>

<h3>Credentials</h3>

<p>Added <code>config/credentials.yml.enc</code> file to store production app secrets. It allows saving any authentication credentials for third-party services directly in repository encrypted with a key in the <code>config/master.key</code> file or the <code>RAILS_MASTER_KEY</code> environment variable</p>

<p>To add new secret to credentials, first run rails secret to get a new secret. Then run rails credentials:edit to edit credentials, and add the secret. Running credentials:edit creates new credentials file and master key, if they did not already exist.</p>

<p>By default, this file contains the application&rsquo;s <code>secret_key_base</code>, but it could also be used to store other credentials such as access keys for external APIs.</p>

<p>The secrets kept in credentials file are accessible via <code>Rails.application.credentials</code>. For example, with the following decrypted <code>config/credentials.yml.enc</code></p>

<p>  #config/credentials.yml.enc</p>

<pre><code>secret_key_base:&lt;secret key base&gt;
api_client_key: &lt;secret key1&gt;
api_client_secret: &lt;secret key2&gt;
</code></pre>

<h3>Content Security Policy</h3>

<p>Content security policy can be configured as a global default policy and then override it on a per-resource basis and even use lambdas to inject per-request values into the header such as account subdomains in a multi-tenant application.</p>

<p>The HTTP <code>Content-Security-Policy</code> response header allows web site administrators to control resources the user agent is allowed to load for a given page. With a few exceptions, policies mostly involve specifying server origins and script endpoints.</p>

<p>Example &ndash;
  # config/initializers/content_security_policy.rb</p>

<pre><code>Rails.application.config.content_security_policy do |policy|
  policy.default_src :self, :https
  policy.font_src    :self, :https, :data
  policy.img_src     :self, :https, :data
  policy.object_src  :none
  policy.script_src  :self, :https
  policy.style_src   :self, :https

  # Specify URI for violation reports
  policy.report_uri "/csp-violation-report-path"
end
</code></pre>

<p>Example controller overrides:</p>

<pre><code># Override policy inline
  class PostsController &lt; ApplicationController
    content_security_policy do |p|
      p.upgrade_insecure_requests true
    end
  end

# Using literal values
  class PostsController &lt; ApplicationController
    content_security_policy do |p|
      p.base_uri "https://www.example.com"
    end
  end

# Disabling the global CSP
  class PagesController &lt; ApplicationController
    content_security_policy false, only: :index
  end
</code></pre>

<br/>


<p>References: <a href="https://guides.rubyonrails.org/5_2_release_notes.html">https://guides.rubyonrails.org/5_2_release_notes.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch - mapper_parsing_exception, failed to parse field]]></title>
    <link href="http://vishnuatrai.in/blog/2017/12/20/elasticsearch-mapper-parsing-exception/"/>
    <updated>2017-12-20T15:37:59+05:30</updated>
    <id>http://vishnuatrai.in/blog/2017/12/20/elasticsearch-mapper-parsing-exception</id>
    <content type="html"><![CDATA[<p>We have recently faced issue with Elasticsearch field types and mapping, when if a field is mapped with a type other type can not be indexed for the same field. Elasticsearch indesing mechanism ristrict that. for example we have created a index named <code>users</code> and inserted a record with <code>text</code> type field <code>name</code>. <!--more--></p>

<p><img src="http://vishnuatrai.in/images//posts/elasticsearch1.png" alt="" /></p>

<p>Second record we want to insert same field with another type per say <code>object</code>, it will raise exceptation <code>mapper_parsing_exception, failed to parse field</code>.</p>

<p><img src="http://vishnuatrai.in/images//posts/Elasticsearch2.png" alt="" /></p>

<p>Because with insertation of first record the index will create a mapping and provide the field type <code>text</code> with second record insertation where the type is an object it will not match with the mapping and raise exceptation.</p>

<p><img src="http://vishnuatrai.in/images//posts/elasticsearch3.png" alt="" /></p>

<h3>Simple Hack But Many pitfalls</h3>

<p>Since by nature Elasticsearch doesn&rsquo;t support above scenario, once the mapping is created and type is assigned to a field it will not allow indexing of other type. However in my case we have to store the data in elasticsearch engine, so converted the object to string and that worked. But it will not support query on the converted object and on client side also we will need to convert back to object type.</p>

<p><img src="http://vishnuatrai.in/images//posts/elasticsearch4.png" alt="" /></p>

<p>After converting the object to text it will indexed as a text field only and will look like a escaped string object.</p>

<p><img src="http://vishnuatrai.in/images//posts/elasticsearch5.png" alt="" /></p>

<p>Still investigating on better solution or way to work on this issue.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MongoDB Replica Set - localhost and Ops Manager]]></title>
    <link href="http://vishnuatrai.in/blog/2017/04/21/mongodb-replica-set-localhost-and-ops-manager/"/>
    <updated>2017-04-21T17:28:18+05:30</updated>
    <id>http://vishnuatrai.in/blog/2017/04/21/mongodb-replica-set-localhost-and-ops-manager</id>
    <content type="html"><![CDATA[<h3>Setup Local DevEnv Replica Set</h3>

<ol>
<li><p>update <code>mongod.conf</code> for replication</p>

<pre><code> #edit /usr/local/etc/mongod.conf
 systemLog:
     destination: file
     path: /usr/local/var/log/mongodb/mongo.log
     logAppend: true
 storage:
     dbPath: /usr/local/var/mongodb
 net:
     bindIp: 127.0.0.1
 replication:
     replSetName: rs0
     oplogSizeMB: 100
</code></pre></li>
</ol>


<!--more-->


<ol>
<li><p>Restart mongodb service</p>

<pre><code> brew services restart mongodb
</code></pre>

<p> <br/></p></li>
<li><p>log on to <code>mongo</code> shell and initiate replica set</p>

<pre><code> rs.initiate({_id: "rs0", members: [{_id: 0, host: "127.0.0.1:27017"}] })
</code></pre>

<p>verify replication members with <code>rs.status()</code></p></li>
</ol>


<h3>Mongodb Ops Manager</h3>

<p>Ops Manager is a management application from mongodb and its available as part of MongoDB Enterprise. Ops Manager enables to configure and maintain MongoDB nodes and clusters.</p>

<p><strong><em>Server Pool</em></strong></p>

<p>Ops Manager Server Pool allows Ops Manager users with administrative privileges, i.e. Ops Manager Administrators, to maintain a pool of provisioned servers that already have installed. When users in a project want to create a new MongoDB deployment, they can request servers from this pool to host the MongoDB deployment.</p>

<p>If you manage large or multiple MongoDB clusters, you probably find yourself scaling them up, and perhaps down, on demand. And if you’re providing MongoDB as a service to your developer teams, scaling requirements might come with little to no notice.</p>

<p>With a few clicks in the Ops Manager GUI, or via a simple API call, you can add new nodes to a cluster, and remove then when they are no longer needed. MongoDB automatically rebalances data as your topology changes, all without service impact.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rails Applicatoin - Openshift 3.0 deployment]]></title>
    <link href="http://vishnuatrai.in/blog/2016/12/05/rails-applicatoin-openshift-3-dot-0-deployment/"/>
    <updated>2016-12-05T18:34:50+05:30</updated>
    <id>http://vishnuatrai.in/blog/2016/12/05/rails-applicatoin-openshift-3-dot-0-deployment</id>
    <content type="html"><![CDATA[<h3>Prepare helper scripts for application setup, app start and httpd config</h3>

<p>Helper scripts can be stored as part of application code in directory <code>scripts</code> in project root directory.</p>

<p>Sample application setup script</p>

<pre><code># scripts/application_setup.sh

#! /bin/bash
bundle exec rake db:migrate
bundle exec rake db:seed
</code></pre>

<!--more-->


<p></p>

<p>Sample applicaiton startup script</p>

<pre><code># scripts/appStartup.sh

#!/bin/bash
bundle exec rake assets:precompile

config-httpd
exec httpd -D FOREGROUND
</code></pre>

<p>Sample script to configure httpd.conf</p>

<pre><code># scripts/config_httpd.sh

#! /bin/sh

sed -i "s/RailsEnv RAILS_ENV/RailsEnv ${RAILS_ENV}/" /etc/httpd/conf/httpd.conf
echo "PassengerMinInstances 12" &gt;&gt; /etc/httpd/conf/httpd.conf
echo "PassengerMaxPoolSize 12" &gt;&gt; /etc/httpd/conf/httpd.conf
ruby -S passenger-config build-native-support
</code></pre>

<p><code>httpd.conf</code> for passenger apache config will include VirtualHost and passenger configs ie. PassengerRoot, PassengerRuby etc.</p>

<h3>Docker Image build with Rails application setup</h3>

<p>Sample Dockerfile to build image with rails app. Dockerfile will reside in project root directory.</p>

<pre><code>FROM centos/passenger-40-centos7
RUN mkdir -p /opt/app-root/bundle
COPY ./Gemfile /opt/app-root/bundle/Gemfile
COPY ./Gemfile.lock /opt/app-root/bundle/Gemfile.lock

# install sqlite3 on same app image as db, this should be run as seperate docker container service
RUN sudo apt-get install -y sqlite3 libsqlite3-dev

WORKDIR /opt/app-root/bundle
RUN scl enable rh-ruby25 'bundle install --deployment --without capistrano:development:test:int_test --jobs=4'
RUN chmod -R g+w /opt/app-root/bundle/

# Copy application code
COPY . /opt/app-root/src

WORKDIR /opt/app-root/src
# Ensure log directory writable, move in the bundle, 
# set app's default group as root, and put the app startup script in place
RUN chmod -R g+w /opt/app-root/src/log /opt/app-root/src/db /opt/app-root/src/config /opt/app-root/src/public
RUN mkdir -p /opt/app-root/src/tmp
RUN chmod 777 /opt/app-root/src/tmp /opt/app-root/src/log /opt/app-root/src/public
RUN ln -s /opt/app-root/bundle/.bundle /opt/app-root/src/.bundle
RUN mkdir -p /opt/app-root/src/vendor
RUN ln -s /opt/app-root/bundle/vendor/bundle /opt/app-root/src/vendor/bundle

COPY scripts/application_setup.sh /usr/bin/application_setup.sh
COPY scripts/appStartup.sh /usr/bin/appStartup.sh
COPY scripts/config_httpd.sh /usr/bin/config_httpd.sh

COPY scripts/httpd.conf /etc/httpd/conf/httpd.conf

CMD ["/usr/bin/appStartup.sh"]
</code></pre>

<h3>Build Docker image using Dockerfile and push the image to docker image registry</h3>

<h3>Openshift Deployment config</h3>

<p>Using this deployment template we can build a deployment on openshift, these template files can be part of source code in template direcotry of project root dir.</p>

<pre><code>     oc process -f templates/deployment_config.yml | oc create -f -
</code></pre>

<p>Sample  <code>DeploymentConfig</code> template</p>

<pre><code>  # templates/deployment_config.yml

  apiVersion: v1
  kind: DeploymentConfig
  metadata:
    name: sample-rails-app
    labels:
      app: sample-rails-app
  spec:
    strategy:
      type: Recreate
      recreateParams:
        timeoutSeconds: 600
        mid:
          failurePolicy: Abort
          execNewPod:
            command:
            - application_setup.sh
            containerName: sample-rails-app
    triggers:
    - type: ConfigChange
    - type: ImageChange
    replicas: 1
    selector:
      app: sample-rails-app
      deploymentconfig: sample-rails-app
    template:
      metadata:
        name: sample-rails-app
        labels:
          app: sample-rails-app
          deploymentconfig: sample-rails-app
      spec:
        containers:
        - name: sample-rails-app
          image: 'docker.hub.com/railsapp/sample-rails-app:0.1'
          ports:
          - containerPort: 8080
          env:
          - name: RAILS_ENV
            value: dev
        restartPolicy: Always
</code></pre>

<p>Using service template a service can be created that will be mapped to deployment pods.</p>

<pre><code>    oc process -f templates/service_config.yml | oc create -f -
</code></pre>

<p>sample <code>Service</code> template</p>

<pre><code>  # templates/service_config.yml    
  apiVersion: v1
  kind: Service
  metadata:
    name: sample-rails-app
    labels:
      app: sample-rails-app
  spec:
    ports:
    - name: web
      port: 8080
      targetPort: 8080
    selector:
      app: sample-rails-app
      deploymentconfig: sample-rails-app
</code></pre>

<p>Using routes services will be exposed to external clients, service will be mapped to a route and route will be available to external world.</p>

<pre><code>oc process -f templates/route_config.yml | oc create -f -
</code></pre>

<p>sample <code>Route</code> template</p>

<pre><code>  # templates/route_config.yml
  apiVersion: v1
  kind: Route
  apiVersion: v1
  metadata:
    name: sample-rails-app
    labels:
      app: sample-rails-app
  spec:
    host: sample-rails-app.openshift-cluster.com
    to:
      kind: Service
      name: sample-rails-app
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is new in Rails 5]]></title>
    <link href="http://vishnuatrai.in/blog/2016/07/05/what-is-new-in-rails-5/"/>
    <updated>2016-07-05T17:34:19+05:30</updated>
    <id>http://vishnuatrai.in/blog/2016/07/05/what-is-new-in-rails-5</id>
    <content type="html"><![CDATA[<h3>ApplicationRecord</h3>

<p>Similar to <code>ApplicationController</code> which is the common base class for all controllers that you get with new Rails apps, <code>ApplicationRecord</code> will provide a base class for your ActiveRecord models. This provides a common place to put any base model concerns.
<code>app/models/application_record.rb</code> file will be automatically added to models in Rails 5 applications.</p>

<pre><code># app/models/application_record.rb
class ApplicationRecord &lt; ActiveRecord::Base
    self.abstract_class = true
end
</code></pre>

<h3>ActionCable</h3>

<p>Action Cable can integrates websocket with rails application. Action Cable server can handle multiple connection instances. It has only one instance per websocket connection. The client websocker connection(consumer) can subscribe to multiple cable channels.<!--more-->For example Action Cable server can have a <code>ChatChannel</code> and an <code>AppearancesChannel</code> and a consumer(websocket) can subscribe to either one or both of channels.</p>

<pre><code>#Publisher Streams
# app/channels/chat_channel.rb
class ChatChannel &lt; ApplicationCable::Channel
    def subscribed
        stream_from "chat_#{params[:room]}"
    end
end

#Subscriber
// app/javascript/channels/chat_channel.js
import consumer from "./consumer"
consumer.subscriptions.create({ channel: "ChatChannel", room: "Best Room" })
</code></pre>

<h3>ActiveRecord::Attributes</h3>

<p>Define an <code>attribute</code> on a model with type. It is not essential to have a database column with the custom model <code>attribute</code>. <code>attribute</code> can also be used to provide default values.</p>

<pre><code># db/schema.rb
create_table :profiles, force: true do |t|
    t.decimal :gpa
    t.string :name, default: "Full Name"
end

# app/models/profile.rb
class Profile &lt; ActiveRecord::Base
end

profile = Profile.new(gpa: '4.1')

# before
profile.gpa # =&gt; BigDecimal.new(4.1)
profile.new.name # =&gt; "Full Name"

class Profile &lt; ActiveRecord::Base
    attribute :gpa, :integer # custom type
    attribute :name, :string, default: "Your Name" # default value
    attribute :current_time, :datetime, default: -&gt; { Time.now } # default value
    attribute :field_without_db_column, :integer, array: true
end

# after
profile.gpa # =&gt; 10
Profile.new.name # =&gt; "Your Name"
Profile.new.current_time # =&gt; 2015-05-30 11:04:48 -0600
model = Profile.new(field_without_db_column: ["1", "2", "3"])
model.attributes # =&gt; {field_without_db_column: [1, 2, 3]}
</code></pre>

<h3>Rails API &ndash; ActionController::API</h3>

<p>To avoid middlewares used for browser based web applications and server public facing json APIs, we can create API only rails application here after 5.0. <code>rails new my_api --api</code> can be used to generate application which will be API only and will not generate assets and views.</p>

<pre><code># app/controllers/application_controller.rb
class ApplicationController &lt; ActionController::API
end
</code></pre>

<h3>Ruby 2.2.2+ required version from Rails 5.0</h3>

<p>Ruby 2.2.2+ required version from Rails 5.0</p>

<br />


<br />


<h3>References</h3>

<p><a href="https://guides.rubyonrails.org/5_0_release_notes.html">https://guides.rubyonrails.org/5_0_release_notes.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker, Docker compose and Dockerfile]]></title>
    <link href="http://vishnuatrai.in/blog/2015/06/10/docker-docker-compose-dockerfile/"/>
    <updated>2015-06-10T18:29:15+05:30</updated>
    <id>http://vishnuatrai.in/blog/2015/06/10/docker-docker-compose-dockerfile</id>
    <content type="html"><![CDATA[<h3>What is Docker</h3>

<p>The Docker uses the Linux kernel and features of the kernel, like Cgroups and namespaces, to segregate processes so they can run independently. This independence is the intention of containers‐the ability to run multiple processes and apps separately from one another to make better use of your infrastructure while retaining the security you would have with separate systems.</p>

<p>Containers are the organizational units of Docker. When we build an image and start running it; we are running in a container.<!--more--></p>

<p>Unlike the VMs which can communicate with the hardware of the host (ex: Ethernet adapter to create more virtual adapters) Docker containers run in an isolated environment on top of the host&rsquo;s OS.</p>

<h3>What is Docker Compose</h3>

<p>Docker Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration.</p>

<p>Docker Compose lets you bring up a complete development environment with only one command: docker-compose up, and tear it down just as easily using docker-compose down. This allows us developers to keep our development environment in one central place and helps us to easily deploy our applications.</p>

<p>Compose works in all environments: production, staging, development, testing, as well as CI workflows. You can learn more about each case in Common Use Cases.</p>

<p>Using Compose is basically a three-step process:</p>

<pre><code>Define your app’s environment with a Dockerfile so it can be reproduced anywhere.

Define the services that make up your app in docker-compose.yml so they can be run together in an isolated environment.

Run docker-compose up and Compose starts and runs your entire app.
</code></pre>

<p>A docker-compose.yml looks like this:</p>

<pre><code>version: '3'
services:
  web:
    build: .
    ports:
    - "5000:5000"
    volumes:
    - .:/code
    - logvolume01:/var/log
    links:
    - redis
  redis:
    image: redis
volumes:
  logvolume01: {}
</code></pre>

<p>Docker Compose commands &ndash;</p>

<pre><code>$ docker-compose up -d
$ docker-compose down
$ docker-compose start
$ docker-compose stop
$ docker-compose build
$ docker-compose logs -f db
$ docker-compose scale db=4
$ docker-compose events
$ docker-compose exec db bash
</code></pre>

<h3>What is Dockerfile</h3>

<p>Docker can build images automatically by reading the instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image.</p>

<p>The docker build command builds an image from a Dockerfile and a context. The build’s context is the set of files at a specified location PATH or URL. The PATH is a directory on your local filesystem. The URL is a Git repository location.</p>

<p>A context is processed recursively. So, a PATH includes any subdirectories and the URL includes the repository and its submodules. This example shows a build command that uses the current directory as context:</p>

<pre><code>$ docker build .
</code></pre>

<p>The build is run by the Docker daemon, not by the CLI. The first thing a build process does is send the entire context (recursively) to the daemon. In most cases, it’s best to start with an empty directory as context and keep your Dockerfile in that directory. Add only the files needed for building the Dockerfile.</p>

<p>To use a file in the build context, the Dockerfile refers to the file specified in an instruction, for example, a COPY instruction. To increase the build’s performance, exclude files and directories by adding a .dockerignore file to the context directory.</p>

<p>You use the -f flag with docker build to point to a Dockerfile anywhere in your file system.</p>

<pre><code>$ docker build -f /path/to/a/Dockerfile .
</code></pre>

<p><strong><em>Dockerfile Instructions and Commands</em></strong></p>

<p><code>FROM</code>, It defines the base image to use to start the build process.</p>

<p><code>LABEL</code>, add labels to your image to help organize images by project, record licensing information</p>

<p><code>RUN</code>, takes a command as its argument and runs it to form the image. it actually is used to build the image</p>

<p><code>EXPOSE</code>, used to indicate the ports on which a container listens for connections, Its used to associate a specified port to enable networking between the running process inside the container and the outside world.</p>

<p><code>ENV</code>, used to set the environment variables (one or more). These variables consist of “key value” pairs.</p>

<p><code>ADD</code> or <code>COPY</code>, these commands gets two arguments: a source and a destination. It basically copies the files from the source on the host into the container’s own filesystem at the set destination. Although ADD and COPY are functionally similar, COPY is preferred. That’s because it’s more transparent than ADD.</p>

<p><code>CMD</code>, can be used for executing a specific command. it&rsquo;s executed when a container is instantiated using the image being built. It should be considered as an initial, default command that gets executed with the creation of containers based on the image.</p>

<pre><code># Usage 1: CMD application "argument", "argument", ..
FROM ubuntu
CMD echo "This is a test."
</code></pre>

<p><code>ENTRYPOINT</code>, argument sets the concrete default application that is used every time a container is created using the image. For example, if you have installed a specific application inside an image and you will use this image to only run that application, you can state it with ENTRYPOINT and whenever a container is created from that image, your application will be the target.</p>

<p>If you couple ENTRYPOINT with CMD, you can remove “application” from CMD and just leave “arguments” which will be passed to the ENTRYPOINT.</p>

<pre><code># Usage: ENTRYPOINT application "argument", "argument", ..
# Remember: arguments are optional. They can be provided by CMD
#           or during the creation of a container.
ENTRYPOINT echo

# Usage example with CMD:
# Arguments set with CMD can be overridden during *run*
CMD "This is a test."
ENTRYPOINT echo  
</code></pre>

<p>The best use for ENTRYPOINT is to set the image’s main command, allowing that image to be run as though it was that command (and then use CMD as the default flags).</p>

<p>Let’s start with an example of an image for the command line tool s3cmd:</p>

<pre><code>ENTRYPOINT ["s3cmd"]
CMD ["--help"]
</code></pre>

<p>The ENTRYPOINT instruction can also be used in combination with a helper script, allowing it to function in a similar way to the command above, even when starting the tool may require more than one step.</p>

<p>For example, the Postgres Official Image uses the following script as its ENTRYPOINT:</p>

<pre><code>#!/bin/bash
set -e

if [ "$1" = 'postgres' ]; then
    chown -R postgres "$PGDATA"

    if [ -z "$(ls -A "$PGDATA")" ]; then
        gosu postgres initdb
    fi

    exec gosu postgres "$@"
fi

exec "$@"
</code></pre>

<p>The helper script is copied into the container and run via ENTRYPOINT on container start:</p>

<pre><code>COPY ./docker-entrypoint.sh /
ENTRYPOINT ["/docker-entrypoint.sh"]
CMD ["postgres"]
</code></pre>

<p>This script allows the user to interact with Postgres in several ways.</p>

<p>It can simply start Postgres:</p>

<pre><code>$ docker run postgres
</code></pre>

<p>Or, it can be used to run Postgres and pass parameters to the server:</p>

<pre><code>$ docker run postgres postgres --help
</code></pre>

<p>Lastly, it could also be used to start a totally different tool, such as Bash:</p>

<pre><code>$ docker run --rm -it postgres bash
</code></pre>

<p><code>VOLUME</code>, used to expose any storage area, configuration storage, or files/folders created by docker container</p>

<p><code>USER</code>, used to set the UID (or username) which is to run the container based on the image being built.
If a service can run without privileges, use USER to change to a non-root user. Start by creating the user and group in the Dockerfile with something like RUN groupadd -r postgres &amp;&amp; useradd &mdash;no-log-init -r -g postgres postgres.</p>

<pre><code># Usage: USER [UID]
USER 800
</code></pre>

<p>To reduce layers and complexity, avoid switching USER back and forth frequently.</p>

<p><code>WORKDIR</code>, used to set where the command defined with CMD is to be executed</p>

<h3>What is Docker Hub</h3>

<p>Docker Hub is a cloud-hosted version of Docker Registry. A Docker user can opt for Docker Registry, which is a stateless, open source and scalable server-side application, if they prefer to maintain the storage and distribution of Docker images instead of relying on Docker&rsquo;s service.</p>

<h2>References</h2>

<p><a href="https://docs.docker.com/">https://docs.docker.com/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is new in Rails 4.2]]></title>
    <link href="http://vishnuatrai.in/blog/2015/01/15/what-is-new-in-rails-4-dot-2/"/>
    <updated>2015-01-15T17:58:43+05:30</updated>
    <id>http://vishnuatrai.in/blog/2015/01/15/what-is-new-in-rails-4-dot-2</id>
    <content type="html"><![CDATA[<h3>Active Job</h3>

<p>Active Job is a framework for declaring jobs and making them run on a variety of queueing backends. These jobs can be everything from regularly scheduled clean-ups, to billing charges, to mailings. Anything that can be chopped up into small units of work and run in parallel.<!--more--></p>

<pre><code>class GuestsCleanupJob &lt; ActiveJob::Base
  queue_as :default

  def perform(*guests)
    # Do something later
  end
end

# Enqueue a job to be performed as soon the queuing system is free.
GuestsCleanupJob.perform_later guest

#Setting the Backend
# config/application.rb
module YourApp
  class Application &lt; Rails::Application
    # Be sure to have the adapter's gem in your Gemfile and follow
    # the adapter's specific installation and deployment instructions.
    config.active_job.queue_adapter = :sidekiq
  end
end
</code></pre>

<h3>Asynchronous Mails</h3>

<p>Building on top of Active Job, Action Mailer now comes with a <code>deliver_later</code> method that sends emails via the queue, so it doesn&rsquo;t block the controller or model if the queue is asynchronous (the default inline queue blocks).</p>

<p>Sending emails right away is still possible with <code>deliver_now</code>.</p>

<h3>Adequate Record</h3>

<p>Adequate Record is a set of performance improvements in Active Record that makes common find and find_by calls and some association queries up to 2x faster.</p>

<p>It works by caching common SQL queries as prepared statements and reusing them on similar calls, skipping most of the query-generation work on subsequent calls.</p>

<pre><code>Post.find(1)  # First call generates and cache the prepared statement
Post.find(2)  # Subsequent calls reuse the cached prepared statement

Post.find_by_title('first post')
Post.find_by_title('second post')

Post.find_by(title: 'first post')
Post.find_by(title: 'second post')

post.comments
post.comments(true)
</code></pre>

<p>Caching is not used in the following scenarios:</p>

<pre><code>- The model has a default scope
- The model uses single table inheritance
- find with a list of ids, eg.
    Post.find(1, 2, 3)
    Post.find([1,2])
- find_by with SQL fragments, eg.
    Post.find_by('published_at &lt; ?', 2.weeks.ago)
</code></pre>

<h3>Web Console</h3>

<p>Web Console adds an interactive Ruby console on every error page and provides a console view and controller helpers.</p>

<p>The interactive console on error pages lets you execute code in the context of the place where the exception originated. The console helper, if called anywhere in a view or controller, launches an interactive console with the final context, once rendering has completed.</p>

<h3>Foreign Key Support</h3>

<p>The migration DSL now supports adding and removing foreign keys. They are dumped to <code>schema.rb</code> as well. At this time, only the <code>mysql</code>, <code>mysql2</code> and <code>postgresql</code> adapters support foreign keys.</p>

<pre><code># add a foreign key to `articles.author_id` referencing `authors.id`
add_foreign_key :articles, :authors

# add a foreign key to `articles.author_id` referencing `users.lng_id`
add_foreign_key :articles, :users, column: :author_id, primary_key: "lng_id"

# remove the foreign key on `accounts.branch_id`
remove_foreign_key :accounts, :branches

# remove the foreign key on `accounts.owner_id`
remove_foreign_key :accounts, column: :owner_id
</code></pre>

<h3>References</h3>

<p><a href="http://guides.rubyonrails.org/4_2_release_notes.html">http://guides.rubyonrails.org/4_2_release_notes.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ruby 2.2.0 Features]]></title>
    <link href="http://vishnuatrai.in/blog/2014/12/30/ruby-2-dot-2-features/"/>
    <updated>2014-12-30T17:53:04+05:30</updated>
    <id>http://vishnuatrai.in/blog/2014/12/30/ruby-2-dot-2-features</id>
    <content type="html"><![CDATA[<h3>1) Incremental and Symbol GC (RIncGC)</h3>

<p>Ruby 2.2.0 release includes several grabage collection (GC) improvements. Symbols are now garbage collectable.</p>

<p>Following the introduction of generational garbage collection in Ruby 2.1.0, which markedly improved the GC throughput, Ruby maintainers continue to introduce important changes in this space. The generational GC (RGenGC) classifies objects into generations, on the assumption that most objects die young. <!--more--> This assumption allows for high throughput and low pause time on younger objects, because older objects are only evaluated for deletion when there is no memory. But this means that older objects still suffer from high pause time.</p>

<p>The incremental GC (RIncGC), built on top of the generational GC, aims to cut that pause time while maintaining the same throughput. It achieves the shorter pause time by interleaving the mark phase, where objects are marked for GC, with Ruby&rsquo;s regular execution. Before Ruby 2.2.0, the mark phase was done in one big step.</p>

<h3>2) Rails 5.0 support</h3>

<p>Rails 5.0 will target Ruby 2.2+ exclusively. There are a bunch of optimizations coming in Ruby 2.2 that are going to be very nice, but most importantly for Rails, symbols are going to be garbage collected. This means we can shed a lot of weight related to juggling strings when we accept input from the outside world. It also means that we can convert fully to keyword arguments and all the other good stuff from the latest Ruby.</p>

<h3>3) Binding#local_variables</h3>

<p>To know the local variables defined in a scope use <code>binding.local_variables</code> that will give all variables in use.</p>

<pre><code>def m1(a, b)
  puts binding.local_variables
  c = a + b
  puts binding.local_variables
  c
end

irb(main):001&gt; m1(2,3)
[:a, :b, :c]
[:a, :b, :c]
=&gt;5
</code></pre>

<h3>4) Binding#receiver</h3>

<p>Ruby 2.2 provides a way to know which object is receiving the method call using <code>binding.receiver</code> method</p>

<pre><code>class Amimal
  def self.walk
    puts 'walk'
    binding.receiver
  end 
end

irb(main):001&gt; Amimal.walk
walk
=&gt;Animal

class Dog &lt; Animal
end 

irb(main):002&gt; Dog.walk
walk
=&gt;Dog
</code></pre>

<h3>5) Enumerable#slice_after</h3>

<p>This method is a complement to the existing <code>slice_before</code> method.</p>

<p>As the name suggests, <code>slice_before</code> is used to slice and dice enumerables. Given a way to match an element in the enumerable, it will find a match and cut it apart just prior to the match.</p>

<pre><code>irb(main):001&gt;[1, 'a', 2, 'b', 'c', 3, 'd', 'e', 'f'].slice_before { |e| e.is_a?(Integer) }.to_a
=&gt;[[1, "a"], [2, "b", "c"], [3, "d", "e", "f"]]
</code></pre>

<p><code>slice_after</code> does slices after instead:</p>

<pre><code>irb(main):002&gt;[1, 'a', 2, 'b', 'c', 3, 'd', 'e', 'f'].slice_after(Integer).to_a
=&gt;[[1], ["a", 2], ["b", "c", 3], ["d", "e", "f"]]
</code></pre>

<h3>6) Enumerable#slice_when</h3>

<p>A particularly fun addition is <code>slice_when</code>. Unlike <code>slice_after</code>, this method only accepts a block. It walks an enumerable, passing pairs of elements to the block. When the block returns <code>true</code>, the enumerable is sliced between the pair of elements:</p>

<pre><code>irb(main):001&gt;[1, 3, 4, 5, 7, 8, 9, 10, 12].slice_when { |a, b| a + 1 != b }.to_a
=&gt;[[1], [3, 4, 5], [7, 8, 9, 10], [12]]
</code></pre>

<h3>7) Float#next_float, Float#prev_float</h3>

<p>These functions return the next or previous representable float. Note the word “representable” in that sentence, not all floats can be represented.</p>

<pre><code>irb(main):001&gt;1.0.next_float
=&gt;1.0000000000000002

irb(main):002&gt;1.0.prev_float
=&gt;0.9999999999999999
</code></pre>

<h3>8) Kernel#itself</h3>

<p>Ruby went out and got itself an identity method. For those not familiar, an identity method returns the object it’s called on:</p>

<pre><code>irb(main):001&gt;1.itself
=&gt;1

irb(main):002&gt;[2, 3, 3, 1, 2, 3, 3, 1, 1, 2].group_by(&amp;:itself)
=&gt;{2=&gt;[2, 2, 2], 3=&gt;[3, 3, 3, 3], 1=&gt;[1, 1, 1]}
</code></pre>

<h3>8) Method#curry</h3>

<p>You might not have realized that Ruby is capable of currying and partial application. In the past, you could only call <code>curry</code> on a <code>Proc</code>. This same power is now available to you on <code>Method</code>.</p>

<pre><code>def sum(*args)
  args.reduce(:+)
end 

irb(main):001&gt;inc = method(:sum).curry(2).(1)
=&gt;#&lt;Proc:0x007fff322d7420 (lambda)&gt;
irb(main):002&gt;inc.(3)
=&gt;4
</code></pre>

<h3>9) Method#super_method</h3>

<p>Calling <code>super_method</code> returns the method that you would get if you called <code>super</code>. If the method has no parent, it returns <code>nil</code>.</p>

<pre><code>class Cat
  def speak
    'meow'
  end
end

class Tiger &lt; Cat
  def speak
    'roar'
  end
end

irb(main):001&gt;Tiger.new.method('speak')
=&gt;#&lt;Method: Tiger#speak&gt; 
irb(main):002&gt;Tiger.new.method('speak').super_method
=&gt;#&lt;Method: Cat#speak&gt;
irb(main):002&gt;Cat.new.method('speak').super_method
=&gt;nil
</code></pre>

<h3>10) Quoted symbol keys in hashes with a trailing colon</h3>

<p>Ruby 2.2 lets you create quoted symbol keys in hashes with a trailing colon:</p>

<pre><code>irb(main):001&gt;{ 'programming-language': :ruby }
</code></pre>

<h3>References</h3>

<p><a href="https://github.com/ruby/ruby/blob/v2_2_0/NEWS">https://github.com/ruby/ruby/blob/v2_2_0/NEWS</a></p>

<p><a href="https://www.ruby-lang.org/en/news/2014/12/25/ruby-2-2-0-released/">https://www.ruby-lang.org/en/news/2014/12/25/ruby-2-2-0-released/</a></p>

<p><a href="http://www.sitepoint.com/new-methods-ruby-2-2/">http://www.sitepoint.com/new-methods-ruby-2-2/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is new in Rails 4.1]]></title>
    <link href="http://vishnuatrai.in/blog/2014/05/31/what-is-new-in-rails-4-dot-1/"/>
    <updated>2014-05-31T19:59:28+05:30</updated>
    <id>http://vishnuatrai.in/blog/2014/05/31/what-is-new-in-rails-4-dot-1</id>
    <content type="html"><![CDATA[<p>Rails 4.1 is a minor release but includes interesting features below &ndash;</p>

<h3>Spring</h3>

<p>Spring is new application preloader(like spork and zeous) available by default<!--more-->
to rails app. Tests, rake and generators will be running much faster on large apps.</p>

<h3>Secrets</h3>

<p>New file config/secrets.yml for your sensitive data. It contains rails app default
secrets but you can add your secret keys here.</p>

<h3>Action Pack Variants</h3>

<p>Render different views, allows to have different templates and action responses for
the same mime type (say, HTML), even behave differently for phones, tablets and
desktop browsers. You can now have individual templates for the desktop, tablet,
and phone views while sharing all the same controller logic.</p>

<h3>ActionMailer Previews</h3>

<p>Sort of like LetterOpener and MailPreview, provide a way to visually see how emails
look by visiting a special URL that renders them. The preview is available in ie.
<a href="http://localhost:3000/rails/mailers/notifier/welcome,">http://localhost:3000/rails/mailers/notifier/welcome,</a> and a list of them in
<a href="http://localhost:3000/rails/mailers.">http://localhost:3000/rails/mailers.</a> By default, these preview classes live in
test/mailers/previews. This can be configured using the preview_path option.</p>

<h3>ActiveRecord enums</h3>

<p>Declare an enum attribute where the values map to integers in the database, but can
be queried by name. The good: Faster! Probably. It stores integers instead of strings.
You won&rsquo;t notice.</p>

<pre><code>class Conversation &lt; ActiveRecord::Base
  enum status: [ :active, :archived ]
end

conversation.archived!
Conversation.statuses # =&gt; { "active" =&gt; 0, "archived" =&gt; 1 }    
</code></pre>

<h3>Message Verifiers</h3>

<p>Message verifiers can be used to generate and verify signed messages. This can be
useful to safely transport sensitive data like remember-me tokens and friends.
The method Rails.application.message_verifier returns a new message verifier that
signs messages with a key derived from secret_key_base and the given message verifier name:</p>

<pre><code>signed_token = Rails.application.message_verifier(:remember_me).generate(token)
Rails.application.message_verifier(:remember_me).verify(signed_token) # =&gt; token

Rails.application.message_verifier(:remember_me).verify(tampered_token)
# raises ActiveSupport::MessageVerifier::InvalidSignature
</code></pre>

<h3>References</h3>

<p><a href="http://weblog.rubyonrails.org/2014/4/8/Rails-4-1/">http://weblog.rubyonrails.org/2014/4/8/Rails-4-1/</a></p>

<p><a href="http://edgeguides.rubyonrails.org/4_1_release_notes.html">http://edgeguides.rubyonrails.org/4_1_release_notes.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ruby 2.1 Changelog]]></title>
    <link href="http://vishnuatrai.in/blog/2014/04/29/ruby-2-dot-1-changelog/"/>
    <updated>2014-04-29T12:01:04+05:30</updated>
    <id>http://vishnuatrai.in/blog/2014/04/29/ruby-2-dot-1-changelog</id>
    <content type="html"><![CDATA[<p>Ruby 2.1.0 has been released now question is, what&rsquo;s new in ruby 2.1.0. Below list of new
features introduced in ruby 2.1.0.</p>

<h3>1) def&rsquo;s return value <!--more--></h3>

<p>In earlier versions it was nil but in ruby 2.1 it will return a symbol.</p>

<pre><code>irb(main):002:0&gt; def my_method
irb(main):003:1&gt; end
=&gt; :my_method
</code></pre>

<h3>2) Rational Number and Complex Number Literals</h3>

<p>Earlier version you will have to use core classes to use relational and complex numbers,
ruby 2.1 gives r and i suffix for them. Older version will give error for these literals.</p>

<pre><code>irb(main):003:0&gt; (2+3i) + Complex(5, 4i)
=&gt; (3+3i)
</code></pre>

<h3>3) Required Keyword arguments</h3>

<p>Keyword arguments introduced by ruby 2.0 and there was no way to make them required.
In ruby 2.1 for missing keyword arguments it will raise
ArgumentError: missing keyword: keyword</p>

<pre><code>def permaliinkify(str:, delimiter: "-")
  str.downcase.split.join(delimiter)
end

irb(main):002:0&gt; permalinkify(delimiter: "-lol-")
ArgumentError: missing keyword: str
from (irb):LN
from /.rvm/2.1.0/bin/irb:11:in `&lt;main&gt;'
</code></pre>

<h3>4) Restricted Generational Garbage Collector (RGenGC)</h3>

<p>Ruby 2.1 introduced faster garbage collector RGenGC. That will make mark and sweep faster.
RGenGC design of garbage collector leverages the fact that most objects collected
by the garbage collector were the objects most recently created.</p>

<h3>5) Exception#cause</h3>

<p>Ruby 2.1 provides a way to have an exception carry a &ldquo;cause&rdquo; along with it.</p>

<pre><code>begin
  begin
    raise "Error!"
  rescue =&gt; e
    raise StandardError, "take the cause"
  end
rescue Exception =&gt; e
  puts "Caused by  : #{e.cause.message} [#{e.cause.class}]"
end
=&gt; Caused by  : Error! [RuntimeError]
</code></pre>

<h3>6) Refinements</h3>

<p>In ruby 2.1, Refinements provide an alternate way to scope our modifications in
ruby open classes or modules.</p>

<pre><code>module Permalinker
  refine String do
    def permalinkify
      downcase.split.join("-")
    end
  end
end

class Post
  using Permalinker
  def initialize(title)
    @title = title
  end
  def permalink
    @title.permalinkify
  end
end
irb(main):002:0&gt; post = Post.new("Ruby on Rails")
irb(main):002:0&gt; post.permalink
=&gt; "ruby-on-rails"
irb(main):023:0&gt; "ruby on rails".permalinkify
NoMethodError: undefined method `permalinkify' for "ruby on rails":String
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Critical Heartbleed fix for SSL]]></title>
    <link href="http://vishnuatrai.in/blog/2014/04/11/critical-heartbleed-fix-for-ssl/"/>
    <updated>2014-04-11T15:30:33+05:30</updated>
    <id>http://vishnuatrai.in/blog/2014/04/11/critical-heartbleed-fix-for-ssl</id>
    <content type="html"><![CDATA[<p>OpenSSL heartbleed bug allows hackers to untraceably read server traffic and some server memory. This implementation mistake leads to the leak of memory information from the server to the client and from the client to the server. <!--more--></p>

<p>For my Ruby on Rails application deployed on AWS, the minimal steps I took to upgrade fixed OpenSSL version given below &ndash;</p>

<p>  1) Update OpenSSL to 1.0.1g</p>

<pre><code>download source from here http://www.openssl.org/source/openssl-1.0.1g.tar.gz
tar -zxf openssl-1.0.1g.tar.gz 
cd openssl-1.0.1g
./config
make
make test
make install

openssl version

if this will show older version then do below steps

ln -s /usr/local/ssl/bin/openssl /usr/bin/openssl
</code></pre>

<p>  2) Recompile/reinstall ruby with new openssl version</p>

<p>  3) Recompile/reinstall libriaries or gems those are related to openssl</p>

<p>  4) Reboot the server</p>

<p>  5) Regenerate new private key and csr to generate new SSL certificate</p>

<p>  6) Change server access keys and passwords</p>

<p>  7) Change API keys, passwords, tokens</p>

<p>  8) Cahnge session secret key for cookie based session store</p>

<p>  9) Restart the web and app servers</p>

<p>  10) You can ask your application users to change their password</p>
]]></content>
  </entry>
  
</feed>
